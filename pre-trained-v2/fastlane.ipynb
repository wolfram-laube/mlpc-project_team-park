{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-06-13T20:25:12.430543Z"
    }
   },
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import librosa\n",
    "import logging\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load pre-trained tokenizer and model\n",
    "def load_model_and_tokenizer(model_name=\"facebook/wav2vec2-large-960h\"):\n",
    "    logger.info(\"Loading model and tokenizer...\")\n",
    "    processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "    logger.info(\"Model and tokenizer loaded successfully.\")\n",
    "    return processor, model\n",
    "\n",
    "# Extract labels from filenames\n",
    "def extract_labels_from_filename(filename):\n",
    "    match = re.search(r'speech_true_(.*)\\.wav', filename)\n",
    "    if match:\n",
    "        words = match.group(1).split('_')\n",
    "        return ' '.join(words)\n",
    "    return ''\n",
    "\n",
    "# Dataset class\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_files, processor):\n",
    "        self.audio_files = audio_files\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path, audio, sr = self.audio_files[idx]\n",
    "        inputs = self.processor(audio, return_tensors=\"pt\", padding=\"longest\", sampling_rate=sr)\n",
    "        label = extract_labels_from_filename(os.path.basename(file_path))\n",
    "        label_ids = self.processor.tokenizer(label, return_tensors=\"pt\").input_ids\n",
    "        return inputs.input_values.squeeze(), label_ids.squeeze()\n",
    "\n",
    "# Collate function to handle padding in DataLoader\n",
    "def collate_fn(batch):\n",
    "    input_values = [item[0] for item in batch]\n",
    "    label_ids = [item[1] for item in batch]\n",
    "    \n",
    "    input_values = torch.nn.utils.rnn.pad_sequence(input_values, batch_first=True, padding_value=0)\n",
    "    label_ids = torch.nn.utils.rnn.pad_sequence(label_ids, batch_first=True, padding_value=-100)\n",
    "    \n",
    "    return input_values, label_ids\n",
    "\n",
    "# Load audio files\n",
    "def load_audio_files(directory):\n",
    "    audio_data = []\n",
    "    logger.info(f\"Loading audio files from {directory}...\")\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                y, sr = librosa.load(file_path, sr=16000)  # Ensuring consistent sampling rate\n",
    "                audio_data.append((file_path, y, sr))\n",
    "    logger.info(f\"Loaded {len(audio_data)} audio files from {directory}.\")\n",
    "    return audio_data\n",
    "\n",
    "# Training function with validation and model checkpointing\n",
    "def train_model(model, processor, train_loader, val_loader, num_epochs=3, lr=5e-5):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        logger.info(f\"Starting epoch {epoch + 1}/{num_epochs}...\")\n",
    "        with tqdm(total=len(train_loader), desc=f\"Training Epoch {epoch + 1}\") as pbar:\n",
    "            for input_values, label_ids in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(input_values)\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Compute lengths for CTC loss\n",
    "                input_lengths = torch.full((logits.shape[0],), logits.shape[1], dtype=torch.long)\n",
    "                label_lengths = torch.sum(label_ids != -100, dim=1)\n",
    "                \n",
    "                loss = torch.nn.CTCLoss()(logits.transpose(0, 1), label_ids, input_lengths, label_lengths)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                pbar.set_postfix({'loss': loss.item()})\n",
    "                pbar.update(1)\n",
    "        \n",
    "        val_loss = validate_model(model, val_loader)\n",
    "        logger.info(f\"Epoch {epoch + 1} completed. Training Loss: {epoch_loss / len(train_loader):.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Check if this is the best model so far\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            logger.info(\"Saving the new best model...\")\n",
    "            model.save_pretrained(\"fine_tuned_wav2vec2\")\n",
    "            processor.save_pretrained(\"fine_tuned_wav2vec2\")\n",
    "\n",
    "# Validation function\n",
    "def validate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    logger.info(\"Starting validation...\")\n",
    "    with tqdm(total=len(val_loader), desc=\"Validation\") as pbar:\n",
    "        with torch.no_grad():\n",
    "            for input_values, label_ids in val_loader:\n",
    "                outputs = model(input_values)\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Compute lengths for CTC loss\n",
    "                input_lengths = torch.full((logits.shape[0],), logits.shape[1], dtype=torch.long)\n",
    "                label_lengths = torch.sum(label_ids != -100, dim=1)\n",
    "                \n",
    "                loss = torch.nn.CTCLoss()(logits.transpose(0, 1), label_ids, input_lengths, label_lengths)\n",
    "                val_loss += loss.item()\n",
    "                pbar.set_postfix({'loss': loss.item()})\n",
    "                pbar.update(1)\n",
    "    model.train()\n",
    "    return val_loss / len(val_loader)\n",
    "\n",
    "# Inference function with timestamps\n",
    "def infer_with_timestamps(model, processor, audio_file):\n",
    "    y, sr = librosa.load(audio_file, sr=None)\n",
    "    inputs = processor(y, return_tensors=\"pt\", padding=\"longest\", sampling_rate=sr)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs.input_values).logits\n",
    "    \n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "    # Get the frame timestamps\n",
    "    frame_duration = processor.feature_extractor.chunk_length / sr\n",
    "    frame_timestamps = [i * frame_duration for i in range(logits.shape[1])]\n",
    "    \n",
    "    # Align the timestamps with the predicted tokens\n",
    "    word_timestamps = []\n",
    "    current_word = \"\"\n",
    "    current_word_start = None\n",
    "    \n",
    "    for i, token_id in enumerate(predicted_ids[0]):\n",
    "        token = processor.decode([token_id])\n",
    "        if token.strip() != \"\":\n",
    "            if current_word == \"\":\n",
    "                current_word_start = frame_timestamps[i]\n",
    "            current_word += token\n",
    "        else:\n",
    "            if current_word != \"\":\n",
    "                word_timestamps.append((current_word, current_word_start, frame_timestamps[i]))\n",
    "                current_word = \"\"\n",
    "                current_word_start = None\n",
    "    \n",
    "    # Handle last word if any\n",
    "    if current_word != \"\":\n",
    "        word_timestamps.append((current_word, current_word_start, frame_timestamps[-1]))\n",
    "    \n",
    "    return transcription, word_timestamps\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    data_dir = '../dataset'\n",
    "    scenes_path = f'{data_dir}/scenes/wav'\n",
    "    words_path = f'{data_dir}/words'\n",
    "\n",
    "    scenes_audio = load_audio_files(scenes_path)[:10]  # Using only 10 samples for testing\n",
    "    words_audio = load_audio_files(words_path)[:10]  # Using only 10 samples for testing\n",
    "    \n",
    "    processor, model = load_model_and_tokenizer()\n",
    "\n",
    "    val_split = int(len(scenes_audio) * 0.2)\n",
    "    train_dataset = AudioDataset(scenes_audio[val_split:], processor)\n",
    "    val_dataset = AudioDataset(scenes_audio[:val_split], processor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    train_model(model, processor, train_loader, val_loader, num_epochs=3, lr=5e-5)\n",
    "\n",
    "    unseen_audio_file = 'path_to_unseen_audio.wav'\n",
    "    transcription, word_timestamps = infer_with_timestamps(model, processor, unseen_audio_file)\n",
    "\n",
    "    logger.info(\"Transcription: \" + transcription)\n",
    "    logger.info(\"Word Timestamps:\")\n",
    "    for word, start, end in word_timestamps:\n",
    "        logger.info(f\"Word: {word}, Start: {start:.2f}s, End: {end:.2f}s\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading audio files from ../dataset/scenes/wav...\n",
      "INFO:__main__:Loaded 814 audio files from ../dataset/scenes/wav.\n",
      "INFO:__main__:Loading audio files from ../dataset/words...\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "28776ce9376858b4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
