{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wolfram-laube/mlpc-project_team-park/blob/wl/pre-trained/fastlane.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "30ec71c806dc4641"
      },
      "cell_type": "markdown",
      "source": [
        "## All-in-one"
      ],
      "id": "30ec71c806dc4641"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-12T09:53:59.330272Z",
          "start_time": "2024-06-12T09:53:59.317064Z"
        },
        "id": "88c0c58ba28be994"
      },
      "cell_type": "code",
      "source": [
        "data_dir = '../dataset'"
      ],
      "id": "88c0c58ba28be994",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "c45b23e5dc697750"
      },
      "cell_type": "markdown",
      "source": [
        "### audio_filename_utils.py"
      ],
      "id": "c45b23e5dc697750"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-12T09:53:59.420242Z",
          "start_time": "2024-06-12T09:53:59.390254Z"
        },
        "id": "48d3c65f6d41240f"
      },
      "cell_type": "code",
      "source": [
        "# audio_parsing_utils.py\n",
        "\n",
        "import re\n",
        "import unicodedata\n",
        "import logging\n",
        "from collections import defaultdict\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Define the pattern to parse clipped command filenames\n",
        "clipped_command_pattern = re.compile(\n",
        "    r'(\\d+)_speech_(true|false)_((?:[a-zA-ZäöüÄÖÜß]+_(?:an|aus)_?)+)_(\\d+\\.\\d+)_(\\d+\\.\\d+)\\.wav', re.UNICODE\n",
        ")\n",
        "\n",
        "# Define the pattern to parse full scene filenames\n",
        "full_scene_pattern = re.compile(\n",
        "    r'(\\d+)_speech_(true|false)_((?:[a-zA-ZäöüÄÖÜß]+_(?:an|aus)_?)+)\\.wav', re.UNICODE\n",
        ")\n",
        "\n",
        "def normalize_unicode(text):\n",
        "    return unicodedata.normalize('NFC', text)\n",
        "\n",
        "# Function to parse clipped command filenames to extract commands, start time, and end time\n",
        "def parse_clipped_command_filename(filename):\n",
        "    logger.debug(f\"Attempting to parse filename: {filename}\")\n",
        "    filename = normalize_unicode(filename)\n",
        "    match = clipped_command_pattern.match(filename)\n",
        "    if not match:\n",
        "        logger.error(f\"Filename {filename} does not match the expected pattern.\")\n",
        "        raise ValueError(f\"Filename {filename} does not match the expected pattern.\")\n",
        "\n",
        "    # Extract command string and timestamps\n",
        "    commands_str = match.group(3)\n",
        "    start_time = float(match.group(4))\n",
        "    end_time = float(match.group(5))\n",
        "\n",
        "    # Split and format commands\n",
        "    commands = commands_str.split('_')\n",
        "    command_list = []\n",
        "    for i in range(0, len(commands), 2):\n",
        "        command_list.append(f\"{commands[i]} {commands[i+1]}\")\n",
        "\n",
        "    logger.debug(f\"Parsed filename {filename}: file_id={match.group(1)}, speech_flag={match.group(2)}, command_list={command_list}, start_time={start_time}, end_time={end_time}\")\n",
        "    return match.group(1), match.group(2), command_list, start_time, end_time\n",
        "\n",
        "# Function to parse full scene filenames to extract commands\n",
        "def parse_full_scene_filename(filename):\n",
        "    logger.debug(f\"Attempting to parse filename: {filename}\")\n",
        "    filename = normalize_unicode(filename)\n",
        "    match = full_scene_pattern.match(filename)\n",
        "    if not match:\n",
        "        logger.error(f\"Filename {filename} does not match the expected pattern.\")\n",
        "        raise ValueError(f\"Filename {filename} does not match the expected pattern.\")\n",
        "\n",
        "    # Extract command string\n",
        "    file_id = match.group(1)\n",
        "    speech_flag = match.group(2)\n",
        "    commands_str = match.group(3)\n",
        "\n",
        "    # Split and format commands\n",
        "    commands = commands_str.split('_')\n",
        "    command_list = []\n",
        "    for i in range(0, len(commands), 2):\n",
        "        command_list.append(f\"{commands[i]} {commands[i+1]}\")\n",
        "\n",
        "    logger.debug(f\"Parsed filename {filename}: file_id={file_id}, speech_flag={speech_flag}, command_list={command_list}\")\n",
        "    return file_id, speech_flag, command_list\n"
      ],
      "id": "48d3c65f6d41240f",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "cbf941b16bcd9357"
      },
      "cell_type": "markdown",
      "source": [
        "### audio_loading_utils.py"
      ],
      "id": "cbf941b16bcd9357"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-12T09:53:59.434556Z",
          "start_time": "2024-06-12T09:53:59.422405Z"
        },
        "id": "f49a13a7189f9a31"
      },
      "cell_type": "code",
      "source": [
        "# audio_loading_utils.py\n",
        "\n",
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "\"\"\"\n",
        "from audio_parsing_utils import (\n",
        "    parse_clipped_command_filename,\n",
        "    parse_full_scene_filename\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "# Function to load audio files from the scenes directory\n",
        "def load_scene_files(directory):\n",
        "    audio_files = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".wav\"):\n",
        "            filepath = os.path.join(directory, filename)\n",
        "            y, sr = librosa.load(filepath, sr=16000)\n",
        "            y = y.astype(np.float32)  # Ensure all audio data is of type float32\n",
        "            file_id, _, commands = parse_full_scene_filename(filename)\n",
        "            audio_files.append({\"path\": filepath, \"audio\": y, \"sr\": sr, \"text\": \" \".join(commands)})\n",
        "    return audio_files\n",
        "\n",
        "# Function to load audio files from the words directory\n",
        "def load_word_files(directory):\n",
        "    audio_files = []\n",
        "    for root, _, filenames in os.walk(directory):\n",
        "        for filename in filenames:\n",
        "            if filename.endswith(\".wav\"):\n",
        "                filepath = os.path.join(root, filename)\n",
        "                y, sr = librosa.load(filepath, sr=16000)\n",
        "                y = y.astype(np.float32)  # Ensure all audio data is of type float32\n",
        "                text = os.path.basename(root)  # Extract text from folder name\n",
        "                audio_files.append({\"path\": filepath, \"audio\": y, \"sr\": sr, \"text\": text})\n",
        "    return audio_files\n",
        "\n",
        "# Function to load audio files from the clipped commands directory\n",
        "def load_clipped_command_files(directory):\n",
        "    audio_files = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".wav\"):\n",
        "            filepath = os.path.join(directory, filename)\n",
        "            y, sr = librosa.load(filepath, sr=16000)\n",
        "            y = y.astype(np.float32)  # Ensure all audio data is of type float32\n",
        "            _, _, command_list, start_time, end_time = parse_clipped_command_filename(filename)\n",
        "            command = \" \".join(command_list)\n",
        "            audio_files.append({\"path\": filepath, \"audio\": y, \"sr\": sr, \"text\": command, \"start_time\": start_time, \"end_time\": end_time})\n",
        "    return audio_files\n"
      ],
      "id": "f49a13a7189f9a31",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "1cdc3edfe3d64045"
      },
      "cell_type": "markdown",
      "source": [
        "### data_collator.py"
      ],
      "id": "1cdc3edfe3d64045"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-12T09:53:59.444167Z",
          "start_time": "2024-06-12T09:53:59.435946Z"
        },
        "id": "9cfea41c3f7e8269"
      },
      "cell_type": "code",
      "source": [
        "# data_collator.py\n",
        "\n",
        "import torch\n",
        "from transformers import Wav2Vec2Processor\n",
        "\n",
        "class DataCollatorCTCWithPadding:\n",
        "    def __init__(self, processor: Wav2Vec2Processor, padding=True):\n",
        "        self.processor = processor\n",
        "        self.padding = padding\n",
        "\n",
        "    def __call__(self, features):\n",
        "        input_values = [feature[\"input_values\"] for feature in features]\n",
        "        labels = [feature[\"labels\"] for feature in features]\n",
        "\n",
        "        batch = self.processor.pad(\n",
        "            {\"input_values\": input_values},\n",
        "            padding=self.padding,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        with self.processor.as_target_processor():\n",
        "            labels_batch = self.processor.pad(\n",
        "                {\"input_ids\": labels},\n",
        "                padding=self.padding,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "        # Replace padding with -100 to ignore them during loss computation\n",
        "        labels_batch[\"input_ids\"][labels_batch[\"input_ids\"] == self.processor.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        batch[\"labels\"] = labels_batch[\"input_ids\"]\n",
        "\n",
        "        return batch\n"
      ],
      "id": "9cfea41c3f7e8269",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "jupyter": {
          "is_executing": true
        },
        "ExecuteTime": {
          "start_time": "2024-06-12T10:39:20.114229Z"
        },
        "id": "a7e16625c29de281",
        "outputId": "6ab096f9-fa87-4fbc-e13d-8414daa07b7f",
        "colab": {
          "referenced_widgets": [
            "11811c969d764917b5de8d0a4f69edfe",
            "a85088eef7b647cf9a925850cd0b739a"
          ]
        }
      },
      "cell_type": "code",
      "source": [
        "# your_script.py\n",
        "\n",
        "# Install necessary libraries if not already installed\n",
        "!pip install transformers librosa torch datasets noisereduce evaluate jiwer pandas\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, Trainer, TrainingArguments\n",
        "from datasets import Dataset, DatasetDict, load_metric\n",
        "\"\"\"from audio_loading_utils import (\n",
        "    load_scene_files,\n",
        "    load_word_files,\n",
        "    load_clipped_command_files\n",
        ")\n",
        "from data_collator import DataCollatorCTCWithPadding\"\"\"\n",
        "\n",
        "# Set environment variables for MPS fallback and high watermark ratio\n",
        "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
        "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
        "\n",
        "# Set the default tensor type to float32\n",
        "torch.set_default_dtype(torch.float32)\n",
        "\n",
        "# Define directories\n",
        "data_dir = '../dataset'\n",
        "scenes_dir = f'{data_dir}/scenes/wav'\n",
        "words_dir = f'{data_dir}/words'\n",
        "clipped_commands_dir = f'{data_dir}/clipped_commands'\n",
        "\n",
        "# Load datasets\n",
        "scenes_data = load_scene_files(scenes_dir)\n",
        "words_data = load_word_files(words_dir)\n",
        "clipped_commands_data = load_clipped_command_files(clipped_commands_dir)\n",
        "\n",
        "# Function to create a dataset from the audio files\n",
        "def create_dataset(audio_files):\n",
        "    data = {\"path\": [], \"audio\": [], \"text\": []}\n",
        "    for item in audio_files:\n",
        "        data[\"path\"].append(item[\"path\"])\n",
        "        data[\"audio\"].append(item[\"audio\"].tolist())  # Convert numpy array to list\n",
        "        data[\"text\"].append(item[\"text\"])\n",
        "    return Dataset.from_dict(data)\n",
        "\n",
        "# Create datasets\n",
        "scenes_dataset = create_dataset(scenes_data)\n",
        "words_dataset = create_dataset(words_data)\n",
        "clipped_commands_dataset = create_dataset(clipped_commands_data)\n",
        "\n",
        "# Combine datasets into a DatasetDict\n",
        "dataset = DatasetDict({\"train\": scenes_dataset, \"test\": words_dataset})\n",
        "\n",
        "# Load the pre-trained model and processor\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "\n",
        "# Move model to CPU\n",
        "device = torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Preprocess function for dataset\n",
        "def preprocess(batch):\n",
        "    audio = batch[\"audio\"]\n",
        "    batch[\"input_values\"] = processor(audio, sampling_rate=16000).input_values[0]\n",
        "    batch[\"labels\"] = processor.tokenizer(batch[\"text\"], padding=\"max_length\", max_length=128, truncation=True).input_ids\n",
        "    return batch\n",
        "\n",
        "# Apply preprocessing\n",
        "dataset = dataset.map(preprocess, remove_columns=[\"path\", \"audio\", \"text\"])\n",
        "\n",
        "# Define data collator\n",
        "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
        "\n",
        "# Define metric\n",
        "wer_metric = load_metric(\"wer\")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions.argmax(-1)\n",
        "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = processor.batch_decode(pred.label_ids, skip_special_tokens=True)\n",
        "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "    return {\"wer\": wer}\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,  # Reduce batch size\n",
        "    per_device_eval_batch_size=4,  # Reduce batch size\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_steps=10,\n",
        "    save_total_limit=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    report_to=\"none\",  # to disable wandb and other integrations\n",
        "    gradient_accumulation_steps=2,  # Accumulate gradients\n",
        "    fp16=False,  # Ensure mixed precision training is disabled\n",
        "    bf16=False,  # Ensure BF16 precision is disabled\n",
        "    no_cuda=True  # Explicitly disable CUDA\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    tokenizer=processor.feature_extractor,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Evaluation results: {eval_results}\")\n"
      ],
      "id": "a7e16625c29de281",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/site-packages (4.41.2)\r\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/site-packages (0.10.2.post1)\r\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/site-packages (2.0.1)\r\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/site-packages (2.19.2)\r\n",
            "Requirement already satisfied: noisereduce in /usr/local/lib/python3.11/site-packages (3.0.2)\r\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/site-packages (0.4.2)\r\n",
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.11/site-packages (3.0.4)\r\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/site-packages (2.0.0)\r\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from transformers) (3.12.0)\r\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.11/site-packages (from transformers) (0.23.3)\r\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/site-packages (from transformers) (1.24.2)\r\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/site-packages (from transformers) (23.0)\r\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/site-packages (from transformers) (6.0)\r\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/site-packages (from transformers) (2024.5.15)\r\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (from transformers) (2.32.3)\r\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/site-packages (from transformers) (0.19.1)\r\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/site-packages (from transformers) (0.4.3)\r\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/site-packages (from transformers) (4.65.0)\r\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/site-packages (from librosa) (3.0.1)\r\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.11/site-packages (from librosa) (1.11.4)\r\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/site-packages (from librosa) (1.2.2)\r\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/site-packages (from librosa) (1.2.0)\r\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/site-packages (from librosa) (5.1.1)\r\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/site-packages (from librosa) (0.59.1)\r\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/site-packages (from librosa) (0.12.1)\r\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/site-packages (from librosa) (1.8.1)\r\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/site-packages (from librosa) (0.3.7)\r\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/site-packages (from librosa) (4.5.0)\r\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/site-packages (from librosa) (0.4)\r\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/site-packages (from librosa) (1.0.8)\r\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/site-packages (from torch) (1.11.1)\r\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/site-packages (from torch) (3.1)\r\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from torch) (3.1.2)\r\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.11/site-packages (from datasets) (16.1.0)\r\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.11/site-packages (from datasets) (0.6)\r\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/site-packages (from datasets) (0.3.8)\r\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/site-packages (from datasets) (3.4.1)\r\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/site-packages (from datasets) (0.70.16)\r\n",
            "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.11/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2023.10.0)\r\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/site-packages (from datasets) (3.9.5)\r\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/site-packages (from noisereduce) (3.7.1)\r\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.11/site-packages (from jiwer) (8.1.6)\r\n",
            "Requirement already satisfied: rapidfuzz<4,>=3 in /usr/local/lib/python3.11/site-packages (from jiwer) (3.9.3)\r\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/site-packages (from pandas) (2.8.2)\r\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas) (2023.3)\r\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/site-packages (from pandas) (2023.3)\r\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\r\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets) (22.2.0)\r\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\r\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\r\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\r\n",
            "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /usr/local/lib/python3.11/site-packages (from numba>=0.51.0->librosa) (0.42.0)\r\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/site-packages (from pooch>=1.1->librosa) (3.1.0)\r\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\r\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (3.1.0)\r\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (3.4)\r\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\r\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (2023.5.7)\r\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/site-packages (from scikit-learn>=0.20.0->librosa) (3.1.0)\r\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/site-packages (from soundfile>=0.12.1->librosa) (1.15.1)\r\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2->torch) (2.1.2)\r\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/site-packages (from matplotlib->noisereduce) (1.0.7)\r\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/site-packages (from matplotlib->noisereduce) (0.11.0)\r\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/site-packages (from matplotlib->noisereduce) (4.39.3)\r\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/site-packages (from matplotlib->noisereduce) (1.4.4)\r\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/site-packages (from matplotlib->noisereduce) (10.0.0)\r\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/site-packages (from matplotlib->noisereduce) (3.0.9)\r\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\r\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\r\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/site-packages/transformers/configuration_utils.py:364: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/814 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "11811c969d764917b5de8d0a4f69edfe"
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/45296 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a85088eef7b647cf9a925850cd0b739a"
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/site-packages/datasets/load.py:759: FutureWarning: The repository for wer contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.2/metrics/wer/wer.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/transformers/training_args.py:1489: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='28' max='306' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 28/306 1:09:44 < 12:25:40, 0.01 it/s, Epoch 0.26/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "64c9ba29cc4e0297"
      },
      "cell_type": "code",
      "source": [
        "model_dir = f\"{data_dir}/meta\"\n",
        "processor_dir = f\"{data_dir}/meta\"\n",
        "\n",
        "# Check if existing model and processor exist\n",
        "if os.path.exists(model_dir) and os.path.exists(processor_dir):\n",
        "    # Load the existing model and processor\n",
        "    existing_model = Wav2Vec2ForCTC.from_pretrained(model_dir)\n",
        "    existing_processor = Wav2Vec2Processor.from_pretrained(processor_dir)\n",
        "    existing_model.to(device)\n",
        "\n",
        "    # Initialize a new Trainer for the existing model\n",
        "    existing_trainer = Trainer(\n",
        "        model=existing_model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset[\"train\"],\n",
        "        eval_dataset=dataset[\"test\"],\n",
        "        tokenizer=existing_processor.feature_extractor,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Evaluate the existing model\n",
        "    existing_eval_results = existing_trainer.evaluate()\n",
        "    print(f\"Existing model evaluation results: {existing_eval_results}\")\n",
        "\n",
        "    # Compare WER (lower is better)\n",
        "    if new_eval_results[\"eval_wer\"] < existing_eval_results[\"eval_wer\"]:\n",
        "        print(\"New model performs better. Saving new model and processor.\")\n",
        "        model.save_pretrained(model_dir)\n",
        "        processor.save_pretrained(processor_dir)\n",
        "    else:\n",
        "        print(\"Existing model performs better. Keeping existing model and processor.\")\n",
        "else:\n",
        "    print(\"No existing model found. Saving new model and processor.\")\n",
        "    model.save_pretrained(model_dir)\n",
        "    processor.save_pretrained(processor_dir)"
      ],
      "id": "64c9ba29cc4e0297",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "fc846265637f6506"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# inference_script.py\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import librosa\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "\n",
        "# Define the directory where the model and processor are saved\n",
        "model_dir = \"path/to/save/model\"\n",
        "processor_dir = \"path/to/save/processor\"\n",
        "\n",
        "# Load the saved model and processor\n",
        "model = Wav2Vec2ForCTC.from_pretrained(model_dir)\n",
        "processor = Wav2Vec2Processor.from_pretrained(processor_dir)\n",
        "\n",
        "# Move model to CPU\n",
        "device = torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Function to transcribe an audio file with timestamps\n",
        "def transcribe(audio_path):\n",
        "    # Load audio file\n",
        "    audio_input, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "    # Preprocess the audio input\n",
        "    input_values = processor(audio_input, return_tensors=\"pt\", padding=\"longest\").input_values.to(device)\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_values).logits\n",
        "\n",
        "    # Decode the logits to text\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    transcription = processor.batch_decode(predicted_ids)[0]\n",
        "\n",
        "    # Get the timestamps\n",
        "    timestamps = []\n",
        "    input_lengths = input_values.shape[1]\n",
        "    time_per_input = len(audio_input) / sr / input_lengths\n",
        "    last_token = None\n",
        "\n",
        "    for i, token in enumerate(predicted_ids[0]):\n",
        "        if token != processor.tokenizer.pad_token_id and token != last_token:\n",
        "            word = processor.tokenizer.decode([token])\n",
        "            start_time = i * time_per_input\n",
        "            end_time = (i + 1) * time_per_input\n",
        "            timestamps.append((word, start_time, end_time))\n",
        "            last_token = token\n",
        "\n",
        "    return transcription, timestamps\n",
        "\n",
        "# Example usage\n",
        "audio_path = \"path/to/audio/file.wav\"\n",
        "transcription, timestamps = transcribe(audio_path)\n",
        "print(f\"Transcription: {transcription}\")\n",
        "for word, start, end in timestamps:\n",
        "    print(f\"Word: {word}, Start time: {start:.2f}, End time: {end:.2f}\")\n"
      ],
      "id": "fc846265637f6506"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}