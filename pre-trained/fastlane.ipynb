{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wolfram-laube/mlpc-project_team-park/blob/wl/pre-trained/fastlane.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "\n",
        "# Check if the environment is Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    # If in Google Colab\n",
        "    from google.colab import drive\n",
        "    import gdown\n",
        "\n",
        "    # Option 1: Download the file by its public link and expand it to the Colab runtime\n",
        "    import urllib.request\n",
        "    import zipfile\n",
        "\n",
        "    scnwavzip_file_id = '1oI1EsH1krrEPbH9MSZRzLHu-_4p6-njR' # https://drive.google.com/file/d/1oI1EsH1krrEPbH9MSZRzLHu-_4p6-njR/view?usp=sharing\n",
        "    scnnpyzip_file_id = '1oKgurvIgT93RGkxvxq8AA423VKlEVT7O' # https://drive.google.com/file/d/1oKgurvIgT93RGkxvxq8AA423VKlEVT7O/view?usp=sharing\n",
        "    wrdwavzip_file_id = '1o1yBqdtqH3tjOHN4GKISJHlY2Qyu_ouX' # https://drive.google.com/file/d/1o1yBqdtqH3tjOHN4GKISJHlY2Qyu_ouX/view?usp=sharing\n",
        "    wrdnpyzip_file_id = '1o2fj6QAM00zg8YMxsHwcNa2lkIXLXDYs' # https://drive.google.com/file/d/1o2fj6QAM00zg8YMxsHwcNa2lkIXLXDYs/view?usp=sharing\n",
        "    annotation_file_id = '1xLxget7c5nCkwYt9Ru2RpYi5rMkk_pl0'  # https://drive.google.com/file/d/1xLxget7c5nCkwYt9Ru2RpYi5rMkk_pl0/view?usp=sharing\n",
        "    scenes_file_id = '1xLgB7-cCz6nReyQbFJJcJGOUKCCbNhCG'  # https://drive.google.com/file/d/1xLgB7-cCz6nReyQbFJJcJGOUKCCbNhCG/view?usp=sharing\n",
        "\n",
        "    scnwavzip_url = f'https://drive.google.com/uc?id={scnwavzip_file_id}'\n",
        "    scnnpyzip_url = f'https://drive.google.com/uc?id={scnnpyzip_file_id}'\n",
        "    wrdwavzip_url = f'https://drive.google.com/uc?id={wrdwavzip_file_id}'\n",
        "    wrdnpyzip_url = f'https://drive.google.com/uc?id={wrdnpyzip_file_id}'\n",
        "    annotation_url = f'https://drive.google.com/uc?id={annotation_file_id}'\n",
        "    scenes_url = f'https://drive.google.com/uc?id={scenes_file_id}'\n",
        "\n",
        "    scnwavzip_path = '/content/scenes_data.zip'\n",
        "    scnnpyzip_path = '/content/scenes_feat.zip'\n",
        "    wrdwavzip_path = '/content/words_data.zip'\n",
        "    wrdnpyzip_path = '/content/words_feat.zip'\n",
        "    data_dir = '/content/dataset'\n",
        "    scenes_dir = f'{data_dir}/scenes'\n",
        "    words_dir = f'{data_dir}/words'\n",
        "    scenes_wav_dir = f'{scenes_dir}/wav'\n",
        "    scenes_npy_dir = f'{scenes_dir}/npy'\n",
        "    words_wav_dir = f'{data_dir}/words'\n",
        "    words_npy_dir = f'{data_dir}/words'\n",
        "\n",
        "    # Download the WAVZIP file\n",
        "    #urllib.request.urlretrieve(wavzip_url, wavzip_path)\n",
        "    gdown.download(scnwavzip_url, scnwavzip_path, quiet=False)\n",
        "\n",
        "    # Unzip the file\n",
        "    with zipfile.ZipFile(scnwavzip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(data_dir)\n",
        "\n",
        "    print(f\"Scenes training data extracted to {data_dir}\")\n",
        "\n",
        "     # Create the 'scenes/wav' folder structure\n",
        "    os.makedirs(scenes_wav_dir, exist_ok=True)\n",
        "\n",
        "    # Copy .wav files to 'scenes/wav'\n",
        "    extracted_scenes_dir = os.path.join(data_dir, 'mlpc24_speech_commands', 'scenes')\n",
        "    for root, dirs, files in os.walk(extracted_scenes_dir):\n",
        "        for file in files:\n",
        "            if file.endswith('.wav'):\n",
        "                src_path = os.path.join(root, file)\n",
        "                dst_path = os.path.join(scenes_wav_dir, file)\n",
        "                shutil.copy(src_path, dst_path)\n",
        "\n",
        "    print(f\"Scenes training .wav files moved to {scenes_wav_dir}\")\n",
        "\n",
        "    # Download the SCNNPYZIP file\n",
        "    gdown.download(scnnpyzip_url, scnnpyzip_path, quiet=False)\n",
        "\n",
        "    # Unzip the file\n",
        "    with zipfile.ZipFile(scnnpyzip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(data_dir)\n",
        "\n",
        "    print(f\"Scenes training features extracted to {data_dir}\")\n",
        "\n",
        "     # Create the 'scenes/npy' folder structure\n",
        "    os.makedirs(scenes_npy_dir, exist_ok=True)\n",
        "\n",
        "    # Copy .npy files to 'scenes/npy'\n",
        "    extracted_scenes_dir = os.path.join(data_dir, 'development_scenes')\n",
        "    for root, dirs, files in os.walk(extracted_scenes_dir):\n",
        "        for file in files:\n",
        "            if file.endswith('.npy'):\n",
        "                src_path = os.path.join(root, file)\n",
        "                dst_path = os.path.join(scenes_npy_dir, file)\n",
        "                shutil.copy(src_path, dst_path)\n",
        "\n",
        "    print(f\"Scenes training .npy files moved to {scenes_npy_dir}\")\n",
        "\n",
        "    # Download the WRDWAVZIP file\n",
        "    #urllib.request.urlretrieve(wavzip_url, wavzip_path)\n",
        "    gdown.download(wrdwavzip_url, wrdwavzip_path, quiet=False)\n",
        "\n",
        "    # Unzip the file\n",
        "    with zipfile.ZipFile(wrdwavzip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(words_wav_dir)\n",
        "\n",
        "    print(f\"Words training data extracted to {words_wav_dir}\")\n",
        "\n",
        "    # Download the WRDNPYZIP file\n",
        "    gdown.download(wrdnpyzip_url, wrdnpyzip_path, quiet=False)\n",
        "\n",
        "    # Unzip the file\n",
        "    with zipfile.ZipFile(wrdnpyzip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(words_npy_dir)\n",
        "\n",
        "    print(f\"Words training ,npy files s extracted to {words_npy_dir}\")\n",
        "\n",
        "\n",
        "    # Download the CSV files into the data_dir\n",
        "    annotation_orig_path = os.path.join(data_dir, 'development_scene_annotations.csv.orig') # Keep a backup copy because it needs fixing\n",
        "    annotation_path = os.path.join(data_dir, 'development_scene_annotations.csv')\n",
        "    scenes_path = os.path.join(data_dir, 'development_scenes.csv')\n",
        "\n",
        "    gdown.download(annotation_url, annotation_orig_path, quiet=False)\n",
        "    gdown.download(annotation_url, annotation_path, quiet=False)\n",
        "    gdown.download(scenes_url, scenes_path, quiet=False)\n",
        "\n",
        "    print(f\"CSV files downloaded to {scenes_dir}\")\n",
        "\n",
        "    # Option 2: Mount Google Drive and use the training data\n",
        "    # Note this really takes some time for preprocessing file by file\n",
        "    #drive.mount('/content/drive')\n",
        "    #data_dir = '/content/drive/My Drive/Dropbox/public/mlpc/dataset'\n",
        "\n",
        "    # Use this option to read from Google Drive instead\n",
        "    #print(f\"Using training data from {data_dir}\")\n",
        "else:\n",
        "    # If on local machine\n",
        "    data_dir = '../dataset'\n",
        "    print(f\"Using local training data from {data_dir}\")\n",
        "\n",
        "# Use the data_dir variable as the path to your training data"
      ],
      "metadata": {
        "id": "JwDs51sUeCQC"
      },
      "id": "JwDs51sUeCQC",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "30ec71c806dc4641"
      },
      "cell_type": "markdown",
      "source": [
        "## All-in-one"
      ],
      "id": "30ec71c806dc4641"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-12T09:53:59.330272Z",
          "start_time": "2024-06-12T09:53:59.317064Z"
        },
        "id": "88c0c58ba28be994"
      },
      "cell_type": "code",
      "source": [
        "data_dir = '/content/dataset'\n",
        "#data_dir = '../dataset'"
      ],
      "id": "88c0c58ba28be994",
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Determine CPU/GPU"
      ],
      "metadata": {
        "id": "gLUlSVPX1-HN"
      },
      "id": "gLUlSVPX1-HN"
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to check if GPU is available\n",
        "#def is_gpu_available():\n",
        "#    try:\n",
        "#        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "#        return result.returncode == 0\n",
        "#    except FileNotFoundError:\n",
        "#        return False\n",
        "\n",
        "def is_gpu_available():\n",
        "    try:\n",
        "        import torch\n",
        "        is_gpu = torch.cuda.is_available()\n",
        "        print(f'GPU available: {is_gpu}')\n",
        "        return is_gpu\n",
        "    except ImportError as ie:\n",
        "        print(\"No GPU support\", ie)\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        import tensorflow as tf\n",
        "        is_gpu =  tf.config.list_physical_devices('GPU') != []\n",
        "        print(f'GPU available: {is_gpu}')\n",
        "        return is_gpu\n",
        "    except ImportError as ie:\n",
        "        print(\"No GPU support\", ie)\n",
        "        pass\n",
        "\n",
        "    print(\"No GPU support found\")\n",
        "    return False\n",
        "\n",
        "is_gpu_available()"
      ],
      "metadata": {
        "id": "cUVQBQoa2Bwo",
        "outputId": "8a90f8cf-4579-488d-df66-726f6ee787b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cUVQBQoa2Bwo",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available: False\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fix erreneous metadata"
      ],
      "metadata": {
        "id": "vHCtYQczy61U"
      },
      "id": "vHCtYQczy61U"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Before"
      ],
      "metadata": {
        "id": "FHub4-6by_8c"
      },
      "id": "FHub4-6by_8c"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV files\n",
        "scene_annotations_df = pd.read_csv(f'{data_dir}/development_scene_annotations.csv')\n",
        "scenes_df = pd.read_csv(f'{data_dir}/development_scenes.csv')\n",
        "\n",
        "# Check the head of the dataframes to understand their structure\n",
        "print(scene_annotations_df.head())\n",
        "print(scenes_df.head())\n",
        "\n",
        "# Check the distribution of labels in the annotations CSV\n",
        "label_distribution_annotations = scene_annotations_df['command'].value_counts()\n",
        "print(\"Label Distribution in development_scene_annotations.csv:\")\n",
        "print(label_distribution_annotations)\n",
        "\n",
        "# Check the distribution of speaker IDs in the scenes CSV\n",
        "label_distribution_scenes = scenes_df['speaker_id'].value_counts()\n",
        "print(\"Label Distribution in development_scenes.csv:\")\n",
        "print(label_distribution_scenes)\n"
      ],
      "metadata": {
        "id": "5z5d_iVyzFl5"
      },
      "id": "5z5d_iVyzFl5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fix"
      ],
      "metadata": {
        "id": "-DukM77OzBbO"
      },
      "id": "-DukM77OzBbO"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import pandas as pd\n",
        "\n",
        "# Paths to the original and working copy files\n",
        "original_file_path = f'{data_dir}/development_scene_annotations.csv.orig'\n",
        "working_copy_path = f'{data_dir}/development_scene_annotations.csv.0'\n",
        "corrected_file_path = f'{data_dir}/development_scene_annotations.csv'\n",
        "\n",
        "# Step 1: Create a working copy of the original file\n",
        "shutil.copy(original_file_path, working_copy_path)\n",
        "\n",
        "# Step 2: Load the working copy into a DataFrame\n",
        "df = pd.read_csv(working_copy_path)\n",
        "\n",
        "# Define the pattern to parse the filename\n",
        "filename_pattern = re.compile(r'(\\d+)_speech_(true|false)_((?:[a-zA-ZäöüÄÖÜß]+_(?:an|aus)_?)+)', re.UNICODE)\n",
        "\n",
        "# Function to parse filename and extract commands\n",
        "def parse_filename(filename):\n",
        "    match = filename_pattern.match(filename)\n",
        "    if not match:\n",
        "        return []\n",
        "\n",
        "    commands_str = match.group(3)\n",
        "    commands = commands_str.split('_')\n",
        "\n",
        "    command_list = []\n",
        "    for i in range(0, len(commands), 2):\n",
        "        command_list.append(f\"{commands[i]} {commands[i+1]}\")\n",
        "\n",
        "    return command_list\n",
        "\n",
        "# Parse the commands from filenames and add to the DataFrame\n",
        "df['parsed_commands'] = df['filename'].apply(parse_filename)\n",
        "\n",
        "# Step 3: Group by filename and sort by start time\n",
        "grouped = df.groupby('filename').apply(lambda x: x.sort_values(by='start')).reset_index(drop=True)\n",
        "\n",
        "# Step 4: Assign the correct labels based on the order of commands in the filename\n",
        "def assign_labels(group):\n",
        "    commands = group['parsed_commands'].iloc[0]  # get the parsed commands from the first row\n",
        "    group = group.reset_index(drop=True)\n",
        "    for i in range(len(group)):\n",
        "        if i < len(commands):\n",
        "            group.at[i, 'command'] = commands[i]\n",
        "        else:\n",
        "            print(f\"Warning: More segments than commands in {group['filename'].iloc[0]}\")\n",
        "    return group\n",
        "\n",
        "# Apply the label assignment function\n",
        "corrected_df = grouped.groupby('filename').apply(assign_labels).reset_index(drop=True)\n",
        "\n",
        "# Drop the temporary column\n",
        "corrected_df = corrected_df.drop(columns=['parsed_commands'])\n",
        "\n",
        "# Step 5: Save the corrected DataFrame to a new CSV file\n",
        "corrected_df.to_csv(corrected_file_path, index=False)\n",
        "\n",
        "# Verify the saved corrections\n",
        "print(\"Label corrections applied and saved successfully.\")\n",
        "print(corrected_df.head())\n"
      ],
      "metadata": {
        "id": "b1Jmhy83zMRi"
      },
      "id": "b1Jmhy83zMRi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### After"
      ],
      "metadata": {
        "id": "6Kt_mH2fzDCc"
      },
      "id": "6Kt_mH2fzDCc"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV files\n",
        "scene_annotations_df = pd.read_csv(f'{data_dir}/development_scene_annotations.csv')\n",
        "scenes_df = pd.read_csv(f'{data_dir}/development_scenes.csv')\n",
        "\n",
        "# Check the head of the dataframes to understand their structure\n",
        "print(scene_annotations_df.head())\n",
        "print(scenes_df.head())\n",
        "\n",
        "# Check the distribution of labels in the annotations CSV\n",
        "label_distribution_annotations = scene_annotations_df['command'].value_counts()\n",
        "print(\"Label Distribution in development_scene_annotations.csv:\")\n",
        "print(label_distribution_annotations)\n",
        "\n",
        "# Check the distribution of speaker IDs in the scenes CSV\n",
        "label_distribution_scenes = scenes_df['speaker_id'].value_counts()\n",
        "print(\"Label Distribution in development_scenes.csv:\")\n",
        "print(label_distribution_scenes)\n"
      ],
      "metadata": {
        "id": "aeJL4mIzzYHs"
      },
      "id": "aeJL4mIzzYHs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess audio data"
      ],
      "metadata": {
        "id": "UjHz2fQ4x9z8"
      },
      "id": "UjHz2fQ4x9z8"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import random\n",
        "from IPython.display import Audio\n",
        "from sklearn.decomposition import FastICA\n",
        "\n",
        "# Function to apply ICA on audio segments\n",
        "def apply_ica(segment, sr):\n",
        "    ica = FastICA(n_components=1, whiten='arbitrary-variance')  # Explicitly set whiten parameter\n",
        "    segment_reshaped = segment.reshape(-1, 1)\n",
        "    segment_ica = ica.fit_transform(segment_reshaped).flatten()\n",
        "    return segment_ica\n",
        "\n",
        "# Function to preprocess segments and optionally save to the filesystem\n",
        "def preprocess_and_save_segments(scenes_dir, annotations_path, save_dir=None, save_to_filesystem=False, apply_ica_flag=False):\n",
        "    # Load the annotations\n",
        "    annotations_df = pd.read_csv(annotations_path)\n",
        "\n",
        "    # Ensure the save directory exists if saving to filesystem\n",
        "    if save_to_filesystem and save_dir is not None:\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    preprocessed_segments = []\n",
        "\n",
        "    for index, row in annotations_df.iterrows():\n",
        "        filename = row['filename']\n",
        "        command = row['command']\n",
        "        start = row['start']\n",
        "        end = row['end']\n",
        "\n",
        "        # Load the audio file\n",
        "        file_path = os.path.join(scenes_dir, f\"{filename}.wav\")\n",
        "        y, sr = librosa.load(file_path, sr=None)\n",
        "\n",
        "        # Extract the segment\n",
        "        start_sample = int(start * sr)\n",
        "        end_sample = int(end * sr)\n",
        "        segment = y[start_sample:end_sample]\n",
        "\n",
        "        # Normalize the segment\n",
        "        segment = librosa.util.normalize(segment)\n",
        "\n",
        "        # Apply ICA if the flag is set\n",
        "        if apply_ica_flag:\n",
        "            segment = apply_ica(segment, sr)\n",
        "\n",
        "        # Add the segment to the list\n",
        "        preprocessed_segments.append((filename, command, segment, sr))\n",
        "\n",
        "        # Save the segment to the filesystem if required\n",
        "        if save_to_filesystem and save_dir is not None:\n",
        "            save_path = os.path.join(save_dir, f\"{filename}_{start}_{end}.wav\")\n",
        "            sf.write(save_path, segment, sr)\n",
        "\n",
        "    return preprocessed_segments\n",
        "\n",
        "# Function to play a random segment from preprocessed segments\n",
        "def play_random_segment(preprocessed_segments):\n",
        "    # Select a random segment\n",
        "    random_segment = random.choice(preprocessed_segments)\n",
        "\n",
        "    filename, command, audio_data, sample_rate = random_segment\n",
        "\n",
        "    # Print the command and play the audio segment\n",
        "    print(f\"Filename: {filename}\")\n",
        "    print(f\"Command: {command}\")\n",
        "\n",
        "    return Audio(audio_data, rate=sample_rate)\n",
        "\n",
        "# Function to play a random segment from the filesystem\n",
        "def play_random_segment_from_filesystem(save_dir, annotations_path):\n",
        "    # List all the preprocessed segment files\n",
        "    segment_files = [f for f in os.listdir(save_dir) if f.endswith('.wav')]\n",
        "\n",
        "    # Select a random segment file\n",
        "    random_segment_file = random.choice(segment_files)\n",
        "    random_segment_path = os.path.join(save_dir, random_segment_file)\n",
        "\n",
        "    # Extract start and end times from the file name\n",
        "    filename_parts = random_segment_file.split('_')\n",
        "    filename = '_'.join(filename_parts[:-2])\n",
        "    start_time = float(filename_parts[-2])\n",
        "    end_time = float(filename_parts[-1].replace('.wav', ''))\n",
        "\n",
        "    # Find the command in the annotations\n",
        "    annotations_df = pd.read_csv(annotations_path)\n",
        "    command_row = annotations_df[\n",
        "        (annotations_df['filename'] == filename) &\n",
        "        (annotations_df['start'] == start_time) &\n",
        "        (annotations_df['end'] == end_time)\n",
        "    ]\n",
        "\n",
        "    if command_row.empty:\n",
        "        print(f\"No matching annotation found for {random_segment_file}\")\n",
        "        return\n",
        "\n",
        "    command = command_row.iloc[0]['command']\n",
        "\n",
        "    # Load the audio segment\n",
        "    y, sr = librosa.load(random_segment_path, sr=None)\n",
        "\n",
        "    # Print the command and play the audio segment\n",
        "    print(f\"Filename: {filename}\")\n",
        "    print(f\"Command: {command}\")\n",
        "\n",
        "    return Audio(y, rate=sr)\n",
        "\n",
        "# Example usage\n",
        "scenes_dir = f'{data_dir}/scenes/wav'\n",
        "annotations_path = f'{data_dir}/development_scene_annotations.csv'\n",
        "save_dir = f'{data_dir}/clipped_commands'\n",
        "\n",
        "# Preprocess segments and save to filesystem with optional ICA\n",
        "preprocessed_segments = preprocess_and_save_segments(scenes_dir, annotations_path, save_dir, save_to_filesystem=True, apply_ica_flag=True)\n",
        "\n",
        "# Play a random segment from memory\n",
        "audio_memory = play_random_segment(preprocessed_segments)\n",
        "display(audio_memory)\n",
        "\n",
        "# Play a random segment from filesystem\n",
        "audio_filesystem = play_random_segment_from_filesystem(save_dir, annotations_path)\n",
        "display(audio_filesystem)\n"
      ],
      "metadata": {
        "id": "4yIlzfGayA2k"
      },
      "id": "4yIlzfGayA2k",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c45b23e5dc697750"
      },
      "cell_type": "markdown",
      "source": [
        "### audio_filename_utils.py"
      ],
      "id": "c45b23e5dc697750"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-12T09:53:59.420242Z",
          "start_time": "2024-06-12T09:53:59.390254Z"
        },
        "id": "48d3c65f6d41240f"
      },
      "cell_type": "code",
      "source": [
        "# audio_parsing_utils.py\n",
        "\n",
        "import re\n",
        "import unicodedata\n",
        "import logging\n",
        "from collections import defaultdict\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Define the pattern to parse clipped command filenames\n",
        "clipped_command_pattern = re.compile(\n",
        "    r'(\\d+)_speech_(true|false)_((?:[a-zA-ZäöüÄÖÜß]+_(?:an|aus)_?)+)_(\\d+\\.\\d+)_(\\d+\\.\\d+)\\.wav', re.UNICODE\n",
        ")\n",
        "\n",
        "# Define the pattern to parse full scene filenames\n",
        "full_scene_pattern = re.compile(\n",
        "    r'(\\d+)_speech_(true|false)_((?:[a-zA-ZäöüÄÖÜß]+_(?:an|aus)_?)+)\\.wav', re.UNICODE\n",
        ")\n",
        "\n",
        "def normalize_unicode(text):\n",
        "    return unicodedata.normalize('NFC', text)\n",
        "\n",
        "# Function to parse clipped command filenames to extract commands, start time, and end time\n",
        "def parse_clipped_command_filename(filename):\n",
        "    logger.debug(f\"Attempting to parse filename: {filename}\")\n",
        "    filename = normalize_unicode(filename)\n",
        "    match = clipped_command_pattern.match(filename)\n",
        "    if not match:\n",
        "        logger.error(f\"Filename {filename} does not match the expected pattern.\")\n",
        "        raise ValueError(f\"Filename {filename} does not match the expected pattern.\")\n",
        "\n",
        "    # Extract command string and timestamps\n",
        "    commands_str = match.group(3)\n",
        "    start_time = float(match.group(4))\n",
        "    end_time = float(match.group(5))\n",
        "\n",
        "    # Split and format commands\n",
        "    commands = commands_str.split('_')\n",
        "    command_list = []\n",
        "    for i in range(0, len(commands), 2):\n",
        "        command_list.append(f\"{commands[i]} {commands[i+1]}\")\n",
        "\n",
        "    logger.debug(f\"Parsed filename {filename}: file_id={match.group(1)}, speech_flag={match.group(2)}, command_list={command_list}, start_time={start_time}, end_time={end_time}\")\n",
        "    return match.group(1), match.group(2), command_list, start_time, end_time\n",
        "\n",
        "# Function to parse full scene filenames to extract commands\n",
        "def parse_full_scene_filename(filename):\n",
        "    logger.debug(f\"Attempting to parse filename: {filename}\")\n",
        "    filename = normalize_unicode(filename)\n",
        "    match = full_scene_pattern.match(filename)\n",
        "    if not match:\n",
        "        logger.error(f\"Filename {filename} does not match the expected pattern.\")\n",
        "        raise ValueError(f\"Filename {filename} does not match the expected pattern.\")\n",
        "\n",
        "    # Extract command string\n",
        "    file_id = match.group(1)\n",
        "    speech_flag = match.group(2)\n",
        "    commands_str = match.group(3)\n",
        "\n",
        "    # Split and format commands\n",
        "    commands = commands_str.split('_')\n",
        "    command_list = []\n",
        "    for i in range(0, len(commands), 2):\n",
        "        command_list.append(f\"{commands[i]} {commands[i+1]}\")\n",
        "\n",
        "    logger.debug(f\"Parsed filename {filename}: file_id={file_id}, speech_flag={speech_flag}, command_list={command_list}\")\n",
        "    return file_id, speech_flag, command_list\n"
      ],
      "id": "48d3c65f6d41240f",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "cbf941b16bcd9357"
      },
      "cell_type": "markdown",
      "source": [
        "### audio_loading_utils.py"
      ],
      "id": "cbf941b16bcd9357"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-12T09:53:59.434556Z",
          "start_time": "2024-06-12T09:53:59.422405Z"
        },
        "id": "f49a13a7189f9a31"
      },
      "cell_type": "code",
      "source": [
        "# audio_loading_utils.py\n",
        "\n",
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "\"\"\"\n",
        "from audio_parsing_utils import (\n",
        "    parse_clipped_command_filename,\n",
        "    parse_full_scene_filename\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "# Function to load audio files from the scenes directory\n",
        "def load_scene_files(directory):\n",
        "    audio_files = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".wav\"):\n",
        "            filepath = os.path.join(directory, filename)\n",
        "            y, sr = librosa.load(filepath, sr=16000)\n",
        "            y = y.astype(np.float32)  # Ensure all audio data is of type float32\n",
        "            file_id, _, commands = parse_full_scene_filename(filename)\n",
        "            audio_files.append({\"path\": filepath, \"audio\": y, \"sr\": sr, \"text\": \" \".join(commands)})\n",
        "    return audio_files\n",
        "\n",
        "# Function to load audio files from the words directory\n",
        "def load_word_files(directory):\n",
        "    audio_files = []\n",
        "    for root, _, filenames in os.walk(directory):\n",
        "        for filename in filenames:\n",
        "            if filename.endswith(\".wav\"):\n",
        "                filepath = os.path.join(root, filename)\n",
        "                y, sr = librosa.load(filepath, sr=16000)\n",
        "                y = y.astype(np.float32)  # Ensure all audio data is of type float32\n",
        "                text = os.path.basename(root)  # Extract text from folder name\n",
        "                audio_files.append({\"path\": filepath, \"audio\": y, \"sr\": sr, \"text\": text})\n",
        "    return audio_files\n",
        "\n",
        "# Function to load audio files from the clipped commands directory\n",
        "def load_clipped_command_files(directory):\n",
        "    audio_files = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".wav\"):\n",
        "            filepath = os.path.join(directory, filename)\n",
        "            y, sr = librosa.load(filepath, sr=16000)\n",
        "            y = y.astype(np.float32)  # Ensure all audio data is of type float32\n",
        "            _, _, command_list, start_time, end_time = parse_clipped_command_filename(filename)\n",
        "            command = \" \".join(command_list)\n",
        "            audio_files.append({\"path\": filepath, \"audio\": y, \"sr\": sr, \"text\": command, \"start_time\": start_time, \"end_time\": end_time})\n",
        "    return audio_files\n"
      ],
      "id": "f49a13a7189f9a31",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "1cdc3edfe3d64045"
      },
      "cell_type": "markdown",
      "source": [
        "### data_collator.py"
      ],
      "id": "1cdc3edfe3d64045"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-12T09:53:59.444167Z",
          "start_time": "2024-06-12T09:53:59.435946Z"
        },
        "id": "9cfea41c3f7e8269"
      },
      "cell_type": "code",
      "source": [
        "# data_collator.py\n",
        "\n",
        "import torch\n",
        "from transformers import Wav2Vec2Processor\n",
        "\n",
        "class DataCollatorCTCWithPadding:\n",
        "    def __init__(self, processor: Wav2Vec2Processor, padding=True):\n",
        "        self.processor = processor\n",
        "        self.padding = padding\n",
        "\n",
        "    def __call__(self, features):\n",
        "        input_values = [feature[\"input_values\"] for feature in features]\n",
        "        labels = [feature[\"labels\"] for feature in features]\n",
        "\n",
        "        batch = self.processor.pad(\n",
        "            {\"input_values\": input_values},\n",
        "            padding=self.padding,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        with self.processor.as_target_processor():\n",
        "            labels_batch = self.processor.pad(\n",
        "                {\"input_ids\": labels},\n",
        "                padding=self.padding,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "        # Replace padding with -100 to ignore them during loss computation\n",
        "        labels_batch[\"input_ids\"][labels_batch[\"input_ids\"] == self.processor.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        batch[\"labels\"] = labels_batch[\"input_ids\"]\n",
        "\n",
        "        return batch\n"
      ],
      "id": "9cfea41c3f7e8269",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "jupyter": {
          "is_executing": true
        },
        "ExecuteTime": {
          "start_time": "2024-06-12T10:39:20.114229Z"
        },
        "id": "a7e16625c29de281"
      },
      "cell_type": "code",
      "source": [
        "# Install necessary libraries if not already installed\n",
        "!pip install transformers librosa torch datasets noisereduce evaluate jiwer pandas\n",
        "\n"
      ],
      "id": "a7e16625c29de281",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, Trainer, TrainingArguments\n",
        "from datasets import Dataset, DatasetDict, load_metric\n",
        "\"\"\"from audio_loading_utils import (\n",
        "    load_scene_files,\n",
        "    load_word_files,\n",
        "    load_clipped_command_files\n",
        ")\n",
        "from data_collator import DataCollatorCTCWithPadding\"\"\"\n",
        "\n",
        "# Set environment variables for MPS fallback and high watermark ratio\n",
        "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
        "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
        "\n",
        "# Set the default tensor type to float32\n",
        "torch.set_default_dtype(torch.float32)\n",
        "\n",
        "# Define directories\n",
        "data_dir = '../dataset'\n",
        "scenes_dir = f'{data_dir}/scenes/wav'\n",
        "words_dir = f'{data_dir}/words'\n",
        "clipped_commands_dir = f'{data_dir}/clipped_commands'\n",
        "\n",
        "# Load datasets\n",
        "scenes_data = load_scene_files(scenes_dir)\n",
        "words_data = load_word_files(words_dir)\n",
        "clipped_commands_data = load_clipped_command_files(clipped_commands_dir)\n",
        "\n",
        "# Function to create a dataset from the audio files\n",
        "def create_dataset(audio_files):\n",
        "    data = {\"path\": [], \"audio\": [], \"text\": []}\n",
        "    for item in audio_files:\n",
        "        data[\"path\"].append(item[\"path\"])\n",
        "        data[\"audio\"].append(item[\"audio\"].tolist())  # Convert numpy array to list\n",
        "        data[\"text\"].append(item[\"text\"])\n",
        "    return Dataset.from_dict(data)\n",
        "\n",
        "# Create datasets\n",
        "scenes_dataset = create_dataset(scenes_data)\n",
        "words_dataset = create_dataset(words_data)\n",
        "clipped_commands_dataset = create_dataset(clipped_commands_data)\n",
        "\n",
        "# Combine datasets into a DatasetDict\n",
        "dataset = DatasetDict({\"train\": scenes_dataset, \"test\": words_dataset})\n",
        "\n",
        "# Load the pre-trained model and processor\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "\n",
        "# Move model to CPU\n",
        "device = torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Preprocess function for dataset\n",
        "def preprocess(batch):\n",
        "    audio = batch[\"audio\"]\n",
        "    batch[\"input_values\"] = processor(audio, sampling_rate=16000).input_values[0]\n",
        "    batch[\"labels\"] = processor.tokenizer(batch[\"text\"], padding=\"max_length\", max_length=128, truncation=True).input_ids\n",
        "    return batch\n",
        "\n",
        "# Apply preprocessing\n",
        "dataset = dataset.map(preprocess, remove_columns=[\"path\", \"audio\", \"text\"])\n",
        "\n",
        "# Define data collator\n",
        "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
        "\n",
        "# Define metric\n",
        "wer_metric = load_metric(\"wer\")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions.argmax(-1)\n",
        "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = processor.batch_decode(pred.label_ids, skip_special_tokens=True)\n",
        "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "    return {\"wer\": wer}\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,  # Reduce batch size\n",
        "    per_device_eval_batch_size=4,  # Reduce batch size\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_steps=10,\n",
        "    save_total_limit=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    report_to=\"none\",  # to disable wandb and other integrations\n",
        "    gradient_accumulation_steps=2,  # Accumulate gradients\n",
        "    fp16=False,  # Ensure mixed precision training is disabled\n",
        "    bf16=False,  # Ensure BF16 precision is disabled\n",
        "    no_cuda=True  # Explicitly disable CUDA\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    tokenizer=processor.feature_extractor,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Evaluation results: {eval_results}\")\n"
      ],
      "metadata": {
        "id": "h-lzNs8T11XQ"
      },
      "id": "h-lzNs8T11XQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "64c9ba29cc4e0297"
      },
      "cell_type": "code",
      "source": [
        "model_dir = f\"{data_dir}/meta\"\n",
        "processor_dir = f\"{data_dir}/meta\"\n",
        "\n",
        "# Check if existing model and processor exist\n",
        "if os.path.exists(model_dir) and os.path.exists(processor_dir):\n",
        "    # Load the existing model and processor\n",
        "    existing_model = Wav2Vec2ForCTC.from_pretrained(model_dir)\n",
        "    existing_processor = Wav2Vec2Processor.from_pretrained(processor_dir)\n",
        "    existing_model.to(device)\n",
        "\n",
        "    # Initialize a new Trainer for the existing model\n",
        "    existing_trainer = Trainer(\n",
        "        model=existing_model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset[\"train\"],\n",
        "        eval_dataset=dataset[\"test\"],\n",
        "        tokenizer=existing_processor.feature_extractor,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Evaluate the existing model\n",
        "    existing_eval_results = existing_trainer.evaluate()\n",
        "    print(f\"Existing model evaluation results: {existing_eval_results}\")\n",
        "\n",
        "    # Compare WER (lower is better)\n",
        "    if new_eval_results[\"eval_wer\"] < existing_eval_results[\"eval_wer\"]:\n",
        "        print(\"New model performs better. Saving new model and processor.\")\n",
        "        model.save_pretrained(model_dir)\n",
        "        processor.save_pretrained(processor_dir)\n",
        "    else:\n",
        "        print(\"Existing model performs better. Keeping existing model and processor.\")\n",
        "else:\n",
        "    print(\"No existing model found. Saving new model and processor.\")\n",
        "    model.save_pretrained(model_dir)\n",
        "    processor.save_pretrained(processor_dir)"
      ],
      "id": "64c9ba29cc4e0297",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "fc846265637f6506"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# inference_script.py\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import librosa\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "\n",
        "# Define the directory where the model and processor are saved\n",
        "model_dir = \"path/to/save/model\"\n",
        "processor_dir = \"path/to/save/processor\"\n",
        "\n",
        "# Load the saved model and processor\n",
        "model = Wav2Vec2ForCTC.from_pretrained(model_dir)\n",
        "processor = Wav2Vec2Processor.from_pretrained(processor_dir)\n",
        "\n",
        "# Move model to CPU\n",
        "device = torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Function to transcribe an audio file with timestamps\n",
        "def transcribe(audio_path):\n",
        "    # Load audio file\n",
        "    audio_input, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "    # Preprocess the audio input\n",
        "    input_values = processor(audio_input, return_tensors=\"pt\", padding=\"longest\").input_values.to(device)\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_values).logits\n",
        "\n",
        "    # Decode the logits to text\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    transcription = processor.batch_decode(predicted_ids)[0]\n",
        "\n",
        "    # Get the timestamps\n",
        "    timestamps = []\n",
        "    input_lengths = input_values.shape[1]\n",
        "    time_per_input = len(audio_input) / sr / input_lengths\n",
        "    last_token = None\n",
        "\n",
        "    for i, token in enumerate(predicted_ids[0]):\n",
        "        if token != processor.tokenizer.pad_token_id and token != last_token:\n",
        "            word = processor.tokenizer.decode([token])\n",
        "            start_time = i * time_per_input\n",
        "            end_time = (i + 1) * time_per_input\n",
        "            timestamps.append((word, start_time, end_time))\n",
        "            last_token = token\n",
        "\n",
        "    return transcription, timestamps\n",
        "\n",
        "# Example usage\n",
        "audio_path = \"path/to/audio/file.wav\"\n",
        "transcription, timestamps = transcribe(audio_path)\n",
        "print(f\"Transcription: {transcription}\")\n",
        "for word, start, end in timestamps:\n",
        "    print(f\"Word: {word}, Start time: {start:.2f}, End time: {end:.2f}\")\n"
      ],
      "id": "fc846265637f6506"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}