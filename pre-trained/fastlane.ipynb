{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## All-in-one",
   "id": "30ec71c806dc4641"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T19:40:56.520296Z",
     "start_time": "2024-06-11T19:40:56.517778Z"
    }
   },
   "cell_type": "code",
   "source": "data_dir = '../dataset'",
   "id": "88c0c58ba28be994",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### audio_filename_utils.py",
   "id": "c45b23e5dc697750"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T19:43:21.153936Z",
     "start_time": "2024-06-11T19:43:21.145876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# audio_parsing_utils.py\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define the pattern to parse clipped command filenames\n",
    "clipped_command_pattern = re.compile(\n",
    "    r'(\\d+)_speech_(true|false)_((?:[a-zA-Z√§√∂√º√Ñ√ñ√ú√ü]+_(?:an|aus)_?)+)_(\\d+\\.\\d+)_(\\d+\\.\\d+)\\.wav', re.UNICODE\n",
    ")\n",
    "\n",
    "# Define the pattern to parse full scene filenames\n",
    "full_scene_pattern = re.compile(\n",
    "    r'(\\d+)_speech_(true|false)_((?:[a-zA-Z√§√∂√º√Ñ√ñ√ú√ü]+_(?:an|aus)_?)+)\\.wav', re.UNICODE\n",
    ")\n",
    "\n",
    "def normalize_unicode(text):\n",
    "    return unicodedata.normalize('NFC', text)\n",
    "\n",
    "# Function to parse clipped command filenames to extract commands, start time, and end time\n",
    "def parse_clipped_command_filename(filename):\n",
    "    logger.debug(f\"Attempting to parse filename: {filename}\")\n",
    "    filename = normalize_unicode(filename)\n",
    "    match = clipped_command_pattern.match(filename)\n",
    "    if not match:\n",
    "        logger.error(f\"Filename {filename} does not match the expected pattern.\")\n",
    "        raise ValueError(f\"Filename {filename} does not match the expected pattern.\")\n",
    "    \n",
    "    # Extract command string and timestamps\n",
    "    commands_str = match.group(3)\n",
    "    start_time = float(match.group(4))\n",
    "    end_time = float(match.group(5))\n",
    "    \n",
    "    # Split and format commands\n",
    "    commands = commands_str.split('_')\n",
    "    command_list = []\n",
    "    for i in range(0, len(commands), 2):\n",
    "        command_list.append(f\"{commands[i]} {commands[i+1]}\")\n",
    "\n",
    "    logger.debug(f\"Parsed filename {filename}: file_id={match.group(1)}, speech_flag={match.group(2)}, command_list={command_list}, start_time={start_time}, end_time={end_time}\")\n",
    "    return match.group(1), match.group(2), command_list, start_time, end_time\n",
    "\n",
    "# Function to parse full scene filenames to extract commands\n",
    "def parse_full_scene_filename(filename):\n",
    "    logger.debug(f\"Attempting to parse filename: {filename}\")\n",
    "    filename = normalize_unicode(filename)\n",
    "    match = full_scene_pattern.match(filename)\n",
    "    if not match:\n",
    "        logger.error(f\"Filename {filename} does not match the expected pattern.\")\n",
    "        raise ValueError(f\"Filename {filename} does not match the expected pattern.\")\n",
    "    \n",
    "    # Extract command string\n",
    "    file_id = match.group(1)\n",
    "    speech_flag = match.group(2)\n",
    "    commands_str = match.group(3)\n",
    "    \n",
    "    # Split and format commands\n",
    "    commands = commands_str.split('_')\n",
    "    command_list = []\n",
    "    for i in range(0, len(commands), 2):\n",
    "        command_list.append(f\"{commands[i]} {commands[i+1]}\")\n",
    "\n",
    "    logger.debug(f\"Parsed filename {filename}: file_id={file_id}, speech_flag={speech_flag}, command_list={command_list}\")\n",
    "    return file_id, speech_flag, command_list\n"
   ],
   "id": "48d3c65f6d41240f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### audio_loading_utils.py",
   "id": "cbf941b16bcd9357"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T19:43:29.661177Z",
     "start_time": "2024-06-11T19:43:29.649414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# audio_loading_utils.py\n",
    "\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "from audio_parsing_utils import (\n",
    "    parse_clipped_command_filename,\n",
    "    parse_full_scene_filename\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Function to load audio files from the scenes directory\n",
    "def load_scene_files(directory):\n",
    "    audio_files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".wav\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            y, sr = librosa.load(filepath, sr=16000)\n",
    "            y = y.astype(np.float32)  # Ensure all audio data is of type float32\n",
    "            file_id, _, commands = parse_full_scene_filename(filename)\n",
    "            audio_files.append({\"path\": filepath, \"audio\": y, \"sr\": sr, \"text\": \" \".join(commands)})\n",
    "    return audio_files\n",
    "\n",
    "# Function to load audio files from the words directory\n",
    "def load_word_files(directory):\n",
    "    audio_files = []\n",
    "    for root, _, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(\".wav\"):\n",
    "                filepath = os.path.join(root, filename)\n",
    "                y, sr = librosa.load(filepath, sr=16000)\n",
    "                y = y.astype(np.float32)  # Ensure all audio data is of type float32\n",
    "                text = os.path.basename(root)  # Extract text from folder name\n",
    "                audio_files.append({\"path\": filepath, \"audio\": y, \"sr\": sr, \"text\": text})\n",
    "    return audio_files\n",
    "\n",
    "# Function to load audio files from the clipped commands directory\n",
    "def load_clipped_command_files(directory):\n",
    "    audio_files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".wav\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            y, sr = librosa.load(filepath, sr=16000)\n",
    "            y = y.astype(np.float32)  # Ensure all audio data is of type float32\n",
    "            _, _, command_list, start_time, end_time = parse_clipped_command_filename(filename)\n",
    "            command = \" \".join(command_list)\n",
    "            audio_files.append({\"path\": filepath, \"audio\": y, \"sr\": sr, \"text\": command, \"start_time\": start_time, \"end_time\": end_time})\n",
    "    return audio_files\n"
   ],
   "id": "f49a13a7189f9a31",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### data_collator.py",
   "id": "1cdc3edfe3d64045"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T19:43:34.228618Z",
     "start_time": "2024-06-11T19:43:31.701086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# data_collator.py\n",
    "\n",
    "import torch\n",
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "class DataCollatorCTCWithPadding:\n",
    "    def __init__(self, processor: Wav2Vec2Processor, padding=True):\n",
    "        self.processor = processor\n",
    "        self.padding = padding\n",
    "\n",
    "    def __call__(self, features):\n",
    "        input_values = [feature[\"input_values\"] for feature in features]\n",
    "        labels = [feature[\"labels\"] for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            {\"input_values\": input_values},\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                {\"input_ids\": labels},\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "        # Replace padding with -100 to ignore them during loss computation\n",
    "        labels_batch[\"input_ids\"][labels_batch[\"input_ids\"] == self.processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        batch[\"labels\"] = labels_batch[\"input_ids\"]\n",
    "\n",
    "        return batch\n"
   ],
   "id": "9cfea41c3f7e8269",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T21:13:33.735212Z",
     "start_time": "2024-06-11T20:57:14.306213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Install necessary libraries if not already installed\n",
    "!pip install transformers librosa torch datasets noisereduce evaluate jiwer pandas\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict, load_metric\n",
    "\"\"\"from audio_loading_utils import (\n",
    "    load_scene_files,\n",
    "    load_word_files,\n",
    "    load_clipped_command_files\n",
    ")\n",
    "from data_collator import DataCollatorCTCWithPadding\"\"\"\n",
    "\n",
    "# Set environment variable for MPS fallback and high watermark ratio\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "# Define directories\n",
    "data_dir = '../dataset'\n",
    "scenes_dir = f'{data_dir}/scenes/wav'\n",
    "words_dir = f'{data_dir}/words'\n",
    "clipped_commands_dir = f'{data_dir}/clipped_commands'\n",
    "\n",
    "# Load datasets\n",
    "scenes_data = load_scene_files(scenes_dir)\n",
    "words_data = load_word_files(words_dir)\n",
    "clipped_commands_data = load_clipped_command_files(clipped_commands_dir)\n",
    "\n",
    "# Function to create a dataset from the audio files\n",
    "def create_dataset(audio_files):\n",
    "    data = {\"path\": [], \"audio\": [], \"text\": []}\n",
    "    for item in audio_files:\n",
    "        data[\"path\"].append(item[\"path\"])\n",
    "        data[\"audio\"].append(item[\"audio\"].tolist())  # Convert numpy array to list\n",
    "        data[\"text\"].append(item[\"text\"])\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "# Create datasets\n",
    "scenes_dataset = create_dataset(scenes_data)\n",
    "words_dataset = create_dataset(words_data)\n",
    "clipped_commands_dataset = create_dataset(clipped_commands_data)\n",
    "\n",
    "# Combine datasets into a DatasetDict\n",
    "dataset = DatasetDict({\"train\": scenes_dataset, \"test\": words_dataset})\n",
    "\n",
    "# Load the pre-trained model and processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "\n",
    "# Move model to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Preprocess function for dataset\n",
    "def preprocess(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    batch[\"input_values\"] = processor(audio, sampling_rate=16000).input_values[0]\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[\"text\"], padding=\"max_length\", max_length=128, truncation=True).input_ids\n",
    "    return batch\n",
    "\n",
    "# Apply preprocessing\n",
    "dataset = dataset.map(preprocess, remove_columns=[\"path\", \"audio\", \"text\"])\n",
    "\n",
    "# Define data collator\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "\n",
    "# Define metric\n",
    "wer_metric = load_metric(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions.argmax(-1)\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.batch_decode(pred.label_ids, skip_special_tokens=True)\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,  # Reduce batch size\n",
    "    per_device_eval_batch_size=4,  # Reduce batch size\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",  # to disable wandb and other integrations\n",
    "    gradient_accumulation_steps=2,  # Accumulate gradients\n",
    "    fp16=False  # Ensure mixed precision training is disabled\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")\n"
   ],
   "id": "a7e16625c29de281",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/site-packages (4.41.2)\r\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.11/site-packages (0.10.2.post1)\r\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/site-packages (2.0.1)\r\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/site-packages (2.19.2)\r\n",
      "Requirement already satisfied: noisereduce in /usr/local/lib/python3.11/site-packages (3.0.2)\r\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/site-packages (0.4.2)\r\n",
      "Requirement already satisfied: jiwer in /usr/local/lib/python3.11/site-packages (3.0.4)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/site-packages (2.0.0)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from transformers) (3.12.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.11/site-packages (from transformers) (0.23.3)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/site-packages (from transformers) (1.24.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/site-packages (from transformers) (23.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/site-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/site-packages (from transformers) (2024.5.15)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (from transformers) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/site-packages (from transformers) (0.19.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/site-packages (from transformers) (0.4.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/site-packages (from transformers) (4.65.0)\r\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/site-packages (from librosa) (3.0.1)\r\n",
      "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.11/site-packages (from librosa) (1.11.4)\r\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/site-packages (from librosa) (1.2.2)\r\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/site-packages (from librosa) (1.2.0)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/site-packages (from librosa) (5.1.1)\r\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/site-packages (from librosa) (0.59.1)\r\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/site-packages (from librosa) (0.12.1)\r\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/site-packages (from librosa) (1.8.1)\r\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/site-packages (from librosa) (0.3.7)\r\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/site-packages (from librosa) (4.5.0)\r\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/site-packages (from librosa) (0.4)\r\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/site-packages (from librosa) (1.0.8)\r\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/site-packages (from torch) (1.11.1)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/site-packages (from torch) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from torch) (3.1.2)\r\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.11/site-packages (from datasets) (16.1.0)\r\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.11/site-packages (from datasets) (0.6)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/site-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/site-packages (from datasets) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/site-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.11/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2023.10.0)\r\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/site-packages (from datasets) (3.9.5)\r\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/site-packages (from noisereduce) (3.7.1)\r\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.11/site-packages (from jiwer) (8.1.6)\r\n",
      "Requirement already satisfied: rapidfuzz<4,>=3 in /usr/local/lib/python3.11/site-packages (from jiwer) (3.9.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas) (2023.3)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/site-packages (from pandas) (2023.3)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets) (22.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\r\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /usr/local/lib/python3.11/site-packages (from numba>=0.51.0->librosa) (0.42.0)\r\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/site-packages (from pooch>=1.1->librosa) (3.1.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (3.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (2023.5.7)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/site-packages (from scikit-learn>=0.20.0->librosa) (3.1.0)\r\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/site-packages (from soundfile>=0.12.1->librosa) (1.15.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2->torch) (2.1.2)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/site-packages (from matplotlib->noisereduce) (1.0.7)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/site-packages (from matplotlib->noisereduce) (0.11.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/site-packages (from matplotlib->noisereduce) (4.39.3)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/site-packages (from matplotlib->noisereduce) (1.4.4)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/site-packages (from matplotlib->noisereduce) (10.0.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/site-packages (from matplotlib->noisereduce) (3.0.9)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\r\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/transformers/configuration_utils.py:364: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/814 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f15702501a04c0b84e5d2bc6308b424"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/45296 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1a20f0ab694041c9ab5d086b6eccde55"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/datasets/load.py:759: FutureWarning: The repository for wer contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.2/metrics/wer/wer.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "fp16 mixed precision requires a GPU (not 'mps').",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 98\u001B[0m\n\u001B[1;32m     80\u001B[0m training_args \u001B[38;5;241m=\u001B[39m TrainingArguments(\n\u001B[1;32m     81\u001B[0m     output_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./results\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     82\u001B[0m     evaluation_strategy\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepoch\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     94\u001B[0m     fp16\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m  \u001B[38;5;66;03m# Ensure mixed precision training is disabled\u001B[39;00m\n\u001B[1;32m     95\u001B[0m )\n\u001B[1;32m     97\u001B[0m \u001B[38;5;66;03m# Initialize Trainer\u001B[39;00m\n\u001B[0;32m---> 98\u001B[0m trainer \u001B[38;5;241m=\u001B[39m \u001B[43mTrainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     99\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    100\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    101\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    102\u001B[0m \u001B[43m    \u001B[49m\u001B[43meval_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtest\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    103\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprocessor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeature_extractor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    104\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_collator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_collator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    105\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompute_metrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompute_metrics\u001B[49m\n\u001B[1;32m    106\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m    108\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[1;32m    109\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/site-packages/transformers/trainer.py:402\u001B[0m, in \u001B[0;36mTrainer.__init__\u001B[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001B[0m\n\u001B[1;32m    399\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdeepspeed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    400\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_in_train \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m--> 402\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_accelerator_and_postprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    404\u001B[0m \u001B[38;5;66;03m# memory metrics - must set up as early as possible\u001B[39;00m\n\u001B[1;32m    405\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_memory_tracker \u001B[38;5;241m=\u001B[39m TrainerMemoryTracker(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mskip_memory_metrics)\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/site-packages/transformers/trainer.py:4535\u001B[0m, in \u001B[0;36mTrainer.create_accelerator_and_postprocess\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   4532\u001B[0m     args\u001B[38;5;241m.\u001B[39mupdate(accelerator_config)\n\u001B[1;32m   4534\u001B[0m \u001B[38;5;66;03m# create accelerator object\u001B[39;00m\n\u001B[0;32m-> 4535\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator \u001B[38;5;241m=\u001B[39m \u001B[43mAccelerator\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4536\u001B[0m \u001B[38;5;66;03m# some Trainer classes need to use `gather` instead of `gather_for_metrics`, thus we store a flag\u001B[39;00m\n\u001B[1;32m   4537\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgather_function \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mgather_for_metrics\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/site-packages/accelerate/accelerator.py:464\u001B[0m, in \u001B[0;36mAccelerator.__init__\u001B[0;34m(self, device_placement, split_batches, mixed_precision, gradient_accumulation_steps, cpu, dataloader_config, deepspeed_plugin, fsdp_plugin, megatron_lm_plugin, rng_types, log_with, project_dir, project_config, gradient_accumulation_plugin, dispatch_batches, even_batches, use_seedable_sampler, step_scheduler_with_optimizer, kwargs_handlers, dynamo_backend)\u001B[0m\n\u001B[1;32m    460\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnative_amp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    461\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxpu\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnpu\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmlu\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m is_torch_xla_available(\n\u001B[1;32m    462\u001B[0m     check_is_tpu\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    463\u001B[0m ):\n\u001B[0;32m--> 464\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfp16 mixed precision requires a GPU (not \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m).\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    465\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscaler_handler\u001B[38;5;241m.\u001B[39mto_kwargs() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscaler_handler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[1;32m    466\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistributed_type \u001B[38;5;241m==\u001B[39m DistributedType\u001B[38;5;241m.\u001B[39mFSDP:\n",
      "\u001B[0;31mValueError\u001B[0m: fp16 mixed precision requires a GPU (not 'mps')."
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "64c9ba29cc4e0297"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
