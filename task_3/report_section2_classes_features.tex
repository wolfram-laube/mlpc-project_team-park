\section{Classes \& Features}

\subsection{Grouping of Words and "Other" Snippets}
The grouping of the 20 keywords and audio snippets into categories was based on their semantic meaning and functionality. This categorization helped in simplifying the classification task by reducing the number of unique classes and aggregating similar concepts together. More general terms are placed in the Miscellaneous category to ensure that the model is not overwhelmed by too many specific categories. This regrouping of categories was applied only to the Random Forest classifier resulting in \ref{tab:keyword_grouping}. For the CNN, the categorization labels were used as-is.

\begin{table}
  \caption{Grouping of Keywords}
  \label{tab:keyword_grouping}
  \centering
  \begin{tabular}{ll}
    \toprule
    Keyword & Group Name \\
    \midrule
    Fernseher & Fernseher \\
    Heizung & Heizung \\
    Lüftung & Lüftung \\
    Ofen & Ofen \\
    Radio & Radio \\
    Staubsauger & Staubsauger \\
    Licht & Licht \\
    Alarm & Alarm \\
    an & Command an \\
    aus & Command aus \\
    warm, offen & Status \\
    Leitung, Spiegel, Brötchen, Haus, Schraube & Objects \\
    kann, nicht, wunderbar, other & Miscellaneous \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Subset of Selected Features}
For the Random Forest classifier, we found that including all available features yielded the best model performance compared to using different feature subsets. Standardizing the features by removing the mean and scaling to unit variance using a standard scaler resulted in a noticeable, though not substantial, improvement in model performance.

\subsection{Preprocessing Steps}
Various preprocessing steps were applied to the data to enhance the quality and effectiveness of the models.

\subsubsection{Random Forest and Nearest Neighbour}
For the Random Forest and Nearest Neighbour classifiers, the precompiled derived features provided in the development file were utilized. These features were preprocessed as follows:
\begin{itemize}
    \item \textbf{Standardization}: The features were standardized by removing the mean and scaling to unit variance using a standard scaler.
    \item \textbf{Log Transformation}: Log transformation was applied to skewed features, though this did not significantly improve performance.
\end{itemize}

\subsubsection{Convolutional Neural Networks (CNN)}
Unlike the other classifiers, the CNN methodology focused solely on raw WAV files, bypassing precompiled feature information to test the effectiveness of raw audio data in training a performant model. The preprocessing steps for the CNN included:
\begin{itemize}
    \item \textbf{Normalization}: The audio data was normalized to ensure that the features were on a similar scale, which is crucial for the effective training of the model.
    \item \textbf{ICA for Noise Reduction}: Independent Component Analysis (ICA) was applied to reduce background noise and enhance signal quality. This step was particularly important for the CNN, which relies heavily on the quality of input data.
\end{itemize}

The CNN architecture consisted of four Conv1D layers with increasing numbers of filters and decreasing kernel sizes. Each Conv1D layer was followed by max-pooling and dropout layers. This was followed by two dense layers with ReLU activation and dropout, and a final softmax output layer.

The training process involved splitting the data into training (70\%), validation (15\%), and test (15\%) sets, using the Adam optimizer with a learning rate set by default. The model was compiled with categorical cross-entropy loss and trained for 20 epochs with a batch size of 32. This puristic approach to using raw audio signals proved highly effective, achieving superior performance compared to the other classifiers.
