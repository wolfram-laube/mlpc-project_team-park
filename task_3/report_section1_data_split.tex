\section{Data Preprocessing}
    \subsection{Data Split}
        The dataset consisted of raw audio recordings of various spoken commands, provided as WAV files. In addition to the raw audio data, a development file containing precompiled derived features was also provided. The data preprocessing involved several critical steps to ensure the quality and consistency of the inputs for the classifiers.

        \subsubsection{Training, Validation, and Test Split}
            The data was split into three distinct sets to facilitate robust evaluation of the classifiers:
            \begin{itemize}
                \item \textbf{Training Set}: Comprised 70\% of the total dataset and was used to train the models.
                \item \textbf{Validation Set}: Comprised 15\% of the total dataset and was used to tune hyperparameters and prevent overfitting.
                \item \textbf{Test Set}: Comprised 15\% of the total dataset and was used to evaluate the final performance of the models.
            \end{itemize}

        \subsubsection{Normalization}
            The audio data was normalized to ensure that the features are on a similar scale, which is crucial for the effective training of the models.

        \subsubsection{ICA for Noise Reduction}
            Independent Component Analysis (ICA) was applied to reduce background noise and enhance signal quality. This step was particularly important for the CNN, which relies heavily on the quality of input data.

        \subsubsection{Precompiled Features}
            The development file contained a set of precompiled derived features for each audio recording. The shape of the provided feature set was (45296, 175, 44), representing 45296 samples with 175 features each spread across 44 time frames. These features were used by some of the classifiers to improve performance.
