{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/wolfram-laube/mlpc-project_team-park/blob/wl/task_3/fastlane.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "id": "1fb74baf4de357fd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup Folder Structure"
   ],
   "metadata": {
    "id": "pnptJBX27tl_"
   },
   "id": "pnptJBX27tl_"
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "# Check if the environment is Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    # If in Google Colab\n",
    "    from google.colab import drive\n",
    "    import gdown\n",
    "\n",
    "    # Option 1: Download the file by its public link and expand it to the Colab runtime\n",
    "    import urllib.request\n",
    "    import zipfile\n",
    "\n",
    "    data_dir = '/content/dataset'\n",
    "\n",
    "    scenes_dir = f'{data_dir}/scenes'\n",
    "    scenes_wav_dir = f'{scenes_dir}/wav'\n",
    "    scenes_npy_dir = f'{scenes_dir}/npy'\n",
    "\n",
    "    words_dir = f'{data_dir}/words'\n",
    "    words_wav_dir = f'{words_dir}/wav'\n",
    "    words_npy_dir = f'{words_dir}/npy'\n",
    "    words_meta_dir = f'{words_dir}/meta'\n",
    "\n",
    "    words_wavzip_file_id = '1o1yBqdtqH3tjOHN4GKISJHlY2Qyu_ouX' # https://drive.google.com/file/d/1o1yBqdtqH3tjOHN4GKISJHlY2Qyu_ouX/view?usp=sharing\n",
    "    words_npyzip_file_id = '1o2fj6QAM00zg8YMxsHwcNa2lkIXLXDYs' # https://drive.google.com/file/d/1o2fj6QAM00zg8YMxsHwcNa2lkIXLXDYs/view?usp=sharing\n",
    "    words_metazip_file_id = '1o4JBl7IeGhcubWz2v0NqcIqWSzxF1i2b'  # https://drive.google.com/file/d/1o4JBl7IeGhcubWz2v0NqcIqWSzxF1i2b/view?usp=sharing\n",
    "    words_scenzip_file_id = '1o7GQf8etjC1TyUB-qx3wDP7VXp3PSbXZ'  # https://drive.google.com/file/d/1o7GQf8etjC1TyUB-qx3wDP7VXp3PSbXZ/view?usp=sharing\n",
    "\n",
    "    words_wavzip_url = f'https://drive.google.com/uc?id={words_wavzip_file_id}'\n",
    "    words_npyzip_url = f'https://drive.google.com/uc?id={words_npyzip_file_id}'\n",
    "    words_metazip_url = f'https://drive.google.com/uc?id={words_metazip_file_id}'\n",
    "    words_scenzip_url = f'https://drive.google.com/uc?id={words_scenzip_file_id}'\n",
    "\n",
    "    words_wavzip_path = '/content/words_data.zip'\n",
    "    words_npyzip_path = '/content/words_feat.zip'\n",
    "    words_metazip_path = '/content/words_meta.zip'\n",
    "    words_scenzip_path = '/content/words_scen.zip'\n",
    "\n",
    "    # Download the WAVZIP file\n",
    "    #urllib.request.urlretrieve(wavzip_url, wavzip_path)\n",
    "    gdown.download(words_wavzip_url, words_wavzip_path, quiet=False)\n",
    "\n",
    "     # Create the 'words/wav' folder structure, each word will have its own subfolder therein\n",
    "    os.makedirs(words_wav_dir, exist_ok=True)\n",
    "\n",
    "    # Unzip the file directly to the target location\n",
    "    # NOTE: Due to inconsistency in metafile development.csv we need to extract\n",
    "    # to the 'words' directory, not 'words/wav' !!!\n",
    "    with zipfile.ZipFile(words_wavzip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(words_dir)\n",
    "\n",
    "    print(f\"Words development data extracted to {words_wav_dir}\")\n",
    "\n",
    "    # Download the NPYZIP file\n",
    "    gdown.download(words_npyzip_url, words_npyzip_path, quiet=False)\n",
    "\n",
    "     # Create the 'words/npy' folder structure\n",
    "    os.makedirs(words_npy_dir, exist_ok=True)\n",
    "\n",
    "    # Unzip the file (consists of a single big NPY file for all words)\n",
    "    with zipfile.ZipFile(words_npyzip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(words_npy_dir)\n",
    "\n",
    "    print(f\"Words development features extracted to {words_npy_dir}\")\n",
    "\n",
    "    # Download the METAZIP file\n",
    "    gdown.download(words_metazip_url, words_metazip_path, quiet=False)\n",
    "\n",
    "     # Create the 'words/meta' folder structure\n",
    "    os.makedirs(words_meta_dir, exist_ok=True)\n",
    "\n",
    "    # Unzip the file (consists of two CSV files)\n",
    "    with zipfile.ZipFile(words_metazip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(words_meta_dir)\n",
    "\n",
    "    print(f\"Words development metainfo extracted to {words_meta_dir}\")\n",
    "\n",
    "else:\n",
    "    # If on local machine\n",
    "    data_dir = '../dataset'\n",
    "\n",
    "# Use the data_dir variable as the path to your development data\n",
    "print(f\"Using development data from {data_dir}\")\n",
    "\n",
    "# For convenience, we define these, too:\n",
    "scenes_dir = f'{data_dir}/scenes'\n",
    "scenes_wav_dir = f'{scenes_dir}/wav'\n",
    "scenes_npy_dir = f'{scenes_dir}/npy'\n",
    "\n",
    "words_dir = f'{data_dir}/words'\n",
    "words_wav_dir = f'{words_dir}/wav'\n",
    "words_npy_dir = f'{words_dir}/npy'\n",
    "words_meta_dir = f'{words_dir}/meta'\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D5QPan2S6YS_",
    "outputId": "23b86416-9b02-4745-9676-469b4fbdbc63",
    "ExecuteTime": {
     "end_time": "2024-06-02T09:28:35.757564Z",
     "start_time": "2024-06-02T09:28:35.751777Z"
    }
   },
   "id": "D5QPan2S6YS_",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Parse the CSV File\n",
    "First, let's inspect the structure of the CSV file to understand its contents."
   ],
   "metadata": {
    "collapsed": false,
    "id": "eef2097c9b092465"
   },
   "id": "eef2097c9b092465"
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file to inspect its structure\n",
    "file_path = f'{words_meta_dir}/development.csv'  # Update with your actual file path\n",
    "metadata = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the metadata\n",
    "print(metadata.head())\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c15913ab22eada0b",
    "outputId": "d09b75c0-0f03-46b0-c103-74606217d2f2",
    "ExecuteTime": {
     "end_time": "2024-06-02T05:06:42.253747Z",
     "start_time": "2024-06-02T05:06:42.201097Z"
    }
   },
   "id": "c15913ab22eada0b",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Load the WAV Files\n",
    "Assuming the CSV contains columns like file_path and label, we can use the librosa library to load the WAV files."
   ],
   "metadata": {
    "collapsed": false,
    "id": "7369b8ca9b3c0d43"
   },
   "id": "7369b8ca9b3c0d43"
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "\n",
    "# Define the root directory where the dataset is located\n",
    "root_dir = f'{data_dir}'  # Replace <root> with the actual path to your root directory\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = f'{words_meta_dir}/development.csv'  # Update with your actual file path\n",
    "metadata = pd.read_csv(file_path)\n",
    "\n",
    "# Function to load a WAV file using the full path\n",
    "def load_wav(file_path):\n",
    "    audio, sr = librosa.load(file_path, sr=None)\n",
    "    return audio, sr\n",
    "\n",
    "# Get the relative file path from the CSV and construct the full path\n",
    "relative_file_path = metadata.loc[0, 'filename']  # Assuming the column name is 'filename'\n",
    "full_file_path = os.path.join(root_dir, relative_file_path)\n",
    "\n",
    "# Load the first audio file\n",
    "audio, sr = load_wav(full_file_path)\n",
    "\n",
    "# Print the shape of the audio array and the sample rate\n",
    "print(audio.shape, sr)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ca7e240efa6d447",
    "outputId": "c7fb5986-56a5-43f9-973e-385880e839b8",
    "ExecuteTime": {
     "end_time": "2024-06-02T05:06:46.723948Z",
     "start_time": "2024-06-02T05:06:44.989268Z"
    }
   },
   "id": "ca7e240efa6d447",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import librosa\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import FastICA\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define the root directory where the dataset is located\n",
    "root_dir = f'{data_dir}'  # Update with your actual path to the dataset directory\n",
    "\n",
    "# Load the CSV file; note this is a little hacky because\n",
    "# initially, a different folder structure was assumed - and hardcoded\n",
    "file_path = os.path.join(root_dir, 'words/meta/development.csv')\n",
    "metadata = pd.read_csv(file_path)\n",
    "\n",
    "# Function to load a WAV file using the full path\n",
    "def load_wav(file_path):\n",
    "    audio, sr = librosa.load(file_path, sr=None)\n",
    "    return audio, sr\n",
    "\n",
    "# Directory to save preprocessed data\n",
    "preprocessed_dir = 'preprocessed_data'\n",
    "os.makedirs(preprocessed_dir, exist_ok=True)\n",
    "\n",
    "# Function to extract the label from the directory name\n",
    "def extract_label(file_path):\n",
    "    # Assuming the structure is f\"{data_dir}/<label>/<filename>.wav\"\n",
    "    return os.path.basename(os.path.dirname(file_path))\n",
    "\n",
    "# Function to check and handle NaNs and Infs\n",
    "def check_and_handle_nans_infs(array):\n",
    "    if np.isnan(array).any() or np.isinf(array).any():\n",
    "        logging.warning(f\"NaNs or Infs found in array: replacing with 0s\")\n",
    "        # Replace NaNs with 0\n",
    "        array = np.nan_to_num(array)\n",
    "        # Replace Infs with 0\n",
    "        array[np.isinf(array)] = 0\n",
    "    return array\n",
    "\n",
    "# Function to preprocess and save all audio files\n",
    "def preprocess_and_save(metadata, root_dir):\n",
    "    scaler = StandardScaler()\n",
    "    ica = FastICA(n_components=1, whiten='unit-variance')\n",
    "\n",
    "    for i, row in metadata.iterrows():\n",
    "        relative_file_path = row['filename']\n",
    "        full_file_path = os.path.join(root_dir, relative_file_path)\n",
    "        label = extract_label(full_file_path)\n",
    "\n",
    "        try:\n",
    "            # Load and preprocess audio\n",
    "            logging.info(f\"Processing file {full_file_path}\")\n",
    "            audio, sr = load_wav(full_file_path)\n",
    "            logging.info(f\"Loaded audio shape: {audio.shape}, sample rate: {sr}\")\n",
    "\n",
    "            # Check for NaNs or Infs in the original audio\n",
    "            audio = check_and_handle_nans_infs(audio)\n",
    "\n",
    "            audio_scaled = scaler.fit_transform(audio.reshape(-1, 1)).flatten()\n",
    "            logging.info(f\"Scaled audio shape: {audio_scaled.shape}\")\n",
    "\n",
    "            # Check for NaNs or Infs in the scaled audio\n",
    "            audio_scaled = check_and_handle_nans_infs(audio_scaled)\n",
    "\n",
    "            audio_ica = ica.fit_transform(audio_scaled.reshape(-1, 1)).flatten()\n",
    "            logging.info(f\"ICA transformed audio shape: {audio_ica.shape}\")\n",
    "\n",
    "            # Save preprocessed audio and label\n",
    "            np.save(os.path.join(preprocessed_dir, f'audio_{i}.npy'), audio_ica)\n",
    "            np.save(os.path.join(preprocessed_dir, f'label_{i}.npy'), label)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing file {full_file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "# Preprocess and save all audio files\n",
    "preprocess_and_save(metadata, root_dir)\n",
    "\n",
    "# Function to load preprocessed data\n",
    "def load_preprocessed_data(preprocessed_dir):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for file_name in os.listdir(preprocessed_dir):\n",
    "        if file_name.startswith('audio'):\n",
    "            audio = np.load(os.path.join(preprocessed_dir, file_name))\n",
    "            label_file = file_name.replace('audio', 'label')\n",
    "            label = np.load(os.path.join(preprocessed_dir, label_file))\n",
    "\n",
    "            X.append(audio)\n",
    "            y.append(label)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load preprocessed data\n",
    "X, y = load_preprocessed_data(preprocessed_dir)\n",
    "\n",
    "# Check the shape of the data\n",
    "print(f\"Shape of data after preprocessing: {X.shape}\")\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f'Training set size: {len(X_train)}')\n",
    "print(f'Validation set size: {len(X_val)}')\n",
    "print(f'Test set size: {len(X_test)}')\n"
   ],
   "metadata": {
    "id": "1548ca6e21f30716",
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-06-01T12:39:44.952808Z"
    }
   },
   "id": "1548ca6e21f30716",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Verify Preprocessed Data\n",
    "Ensure that the preprocessed data was saved correctly and can be loaded for model training.\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "47f9fa67dee45c19"
   },
   "id": "47f9fa67dee45c19"
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Function to load preprocessed data\n",
    "def load_preprocessed_data(preprocessed_dir):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for file_name in os.listdir(preprocessed_dir):\n",
    "        if file_name.startswith('audio'):\n",
    "            audio = np.load(os.path.join(preprocessed_dir, file_name))\n",
    "            label_file = file_name.replace('audio', 'label')\n",
    "            label = np.load(os.path.join(preprocessed_dir, label_file))\n",
    "\n",
    "            X.append(audio)\n",
    "            y.append(label)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load preprocessed data\n",
    "preprocessed_dir = 'preprocessed_data'\n",
    "X, y = load_preprocessed_data(preprocessed_dir)\n",
    "\n",
    "print(f'Loaded {len(X)} preprocessed audio files.')\n",
    "print(f'Labels: {set(y)}')\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f'Training set size: {len(X_train)}')\n",
    "print(f'Validation set size: {len(X_val)}')\n",
    "print(f'Test set size: {len(X_test)}')\n"
   ],
   "metadata": {
    "id": "e11f532c490dca1b",
    "outputId": "c5cc363b-8023-4615-f6ab-ac54d8c2b228",
    "ExecuteTime": {
     "end_time": "2024-06-02T09:24:56.065266Z",
     "start_time": "2024-06-02T09:24:30.750115Z"
    }
   },
   "id": "e11f532c490dca1b",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Train and Evaluate Models\n",
    "Use the preprocessed data to train and evaluate your classifiers.\n",
    "\n",
    "Random Forest"
   ],
   "metadata": {
    "collapsed": false,
    "id": "7f834004d40400eb"
   },
   "id": "7f834004d40400eb"
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Train the Random Forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier on the validation set\n",
    "y_pred_val = rf.predict(X_val)\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "precision_val = precision_score(y_val, y_pred_val, average='weighted')\n",
    "recall_val = recall_score(y_val, y_pred_val, average='weighted')\n",
    "f1_val = f1_score(y_val, y_pred_val, average='weighted')\n",
    "\n",
    "print(f'Random Forest Validation Accuracy: {accuracy_val}')\n",
    "print(f'Random Forest Validation Precision: {precision_val}')\n",
    "print(f'Random Forest Validation Recall: {recall_val}')\n",
    "print(f'Random Forest Validation F1-Score: {f1_val}')\n"
   ],
   "metadata": {
    "id": "7896388f0d55ef57",
    "outputId": "470047d2-48b2-44ce-b682-f5bc45527579",
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "7896388f0d55ef57",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "c42770645de52d5b"
   },
   "cell_type": "markdown",
   "source": [
    "Random Forest Variants:"
   ],
   "id": "c42770645de52d5b"
  },
  {
   "metadata": {
    "id": "f56b2d928ac28945",
    "outputId": "2d008385-22af-4213-9097-945b17a5f04f",
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Random Forest example\n",
    "param_grid = {'n_estimators': [50, 100, 150], 'max_depth': [10, 20, 30]}\n",
    "rf_scores = []\n",
    "\n",
    "for n in param_grid['n_estimators']:\n",
    "    for d in param_grid['max_depth']:\n",
    "        rf = RandomForestClassifier(n_estimators=n, max_depth=d)\n",
    "        rf.fit(X_train, y_train)\n",
    "        y_pred = rf.predict(X_val)\n",
    "        score = accuracy_score(y_val, y_pred)\n",
    "        rf_scores.append((n, d, score))\n",
    "\n",
    "# Visualize the results\n",
    "rf_df = pd.DataFrame(rf_scores, columns=['n_estimators', 'max_depth', 'accuracy'])\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=rf_df, x='n_estimators', y='accuracy', hue='max_depth')\n",
    "plt.title('Random Forest Hyperparameter Tuning')\n",
    "plt.savefig('fig/rf_hyperparameter_tuning.png')\n",
    "plt.show()\n"
   ],
   "id": "f56b2d928ac28945",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "6579d28e805ba586",
    "outputId": "7ab452b9-8925-4f5d-a493-219d17fc69de",
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualize the results\n",
    "rf_df = pd.DataFrame(rf_scores, columns=['n_estimators', 'max_depth', 'accuracy'])\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=rf_df, x='n_estimators', y='accuracy', hue='max_depth')\n",
    "plt.title('Random Forest Hyperparameter Tuning')\n",
    "plt.savefig('fig/rf_hyperparameter_tuning.png')\n",
    "plt.show()"
   ],
   "id": "6579d28e805ba586",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nearest Neighbour"
   ],
   "metadata": {
    "collapsed": false,
    "id": "cf721112eac3bb87"
   },
   "id": "cf721112eac3bb87"
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Train the Nearest Neighbour classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier on the validation set\n",
    "y_pred_val = knn.predict(X_val)\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "precision_val = precision_score(y_val, y_pred_val, average='weighted')\n",
    "recall_val = recall_score(y_val, y_pred_val, average='weighted')\n",
    "f1_val = f1_score(y_val, y_pred_val, average='weighted')\n",
    "\n",
    "print(f'Nearest Neighbour Validation Accuracy: {accuracy_val}')\n",
    "print(f'Nearest Neighbour Validation Precision: {precision_val}')\n",
    "print(f'Nearest Neighbour Validation Recall: {recall_val}')\n",
    "print(f'Nearest Neighbour Validation F1-Score: {f1_val}')\n"
   ],
   "metadata": {
    "id": "33b0202708b55e43",
    "outputId": "a5d8747b-33cb-4aa2-e817-c0cd1fc53407",
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "33b0202708b55e43",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "92d3047c93ca4037"
   },
   "cell_type": "markdown",
   "source": [
    "Nearest Neighbout Variants:"
   ],
   "id": "92d3047c93ca4037"
  },
  {
   "metadata": {
    "id": "2b184c03254ad75d",
    "outputId": "4b21baaf-8565-4d3b-cb21-a0bb995c334c",
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib as plt\n",
    "\n",
    "# Nearest Neighbour example\n",
    "param_grid = {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance']}\n",
    "knn_scores = []\n",
    "\n",
    "for k in param_grid['n_neighbors']:\n",
    "    for w in param_grid['weights']:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, weights=w)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_val)\n",
    "        score = accuracy_score(y_val, y_pred)\n",
    "        knn_scores.append((k, w, score))\n",
    "\n",
    "# Visualize the results\n",
    "knn_df = pd.DataFrame(knn_scores, columns=['n_neighbors', 'weights', 'accuracy'])\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=knn_df, x='n_neighbors', y='accuracy', hue='weights')\n",
    "plt.title('K-Nearest Neighbours Hyperparameter Tuning')\n",
    "plt.savefig('fig/knn_hyperparameter_tuning.png')\n",
    "plt.show()\n"
   ],
   "id": "2b184c03254ad75d",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "37b12f81b6b002b6",
    "outputId": "1f061d7c-9fae-46e4-b44c-fd9e65654a6f",
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Visualize the results\n",
    "knn_df = pd.DataFrame(knn_scores, columns=['n_neighbors', 'weights', 'accuracy'])\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=knn_df, x='n_neighbors', y='accuracy', hue='weights')\n",
    "plt.title('K-Nearest Neighbours Hyperparameter Tuning')\n",
    "plt.savefig('fig/knn_hyperparameter_tuning.png')\n",
    "plt.show()\n"
   ],
   "id": "37b12f81b6b002b6",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "CNN"
   ],
   "metadata": {
    "collapsed": false,
    "id": "dcc3c66d0debe381"
   },
   "id": "dcc3c66d0debe381"
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Print the original shape of the data\n",
    "print(f'Original X_train shape: {X_train.shape}')\n",
    "print(f'Original X_val shape: {X_val.shape}')\n",
    "print(f'Original X_test shape: {X_test.shape}')\n",
    "\n",
    "# Reshape data for 1D CNN\n",
    "X_train_cnn = X_train.reshape(-1, X_train.shape[1], 1)\n",
    "X_val_cnn = X_val.reshape(-1, X_val.shape[1], 1)\n",
    "X_test_cnn = X_test.reshape(-1, X_test.shape[1], 1)\n",
    "\n",
    "# Print the reshaped data shape\n",
    "print(f'Reshaped X_train_cnn shape: {X_train_cnn.shape}')\n",
    "print(f'Reshaped X_val_cnn shape: {X_val_cnn.shape}')\n",
    "print(f'Reshaped X_test_cnn shape: {X_test_cnn.shape}')\n",
    "\n",
    "# Encode labels as numeric\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Convert labels to categorical\n",
    "num_classes = len(np.unique(y_train_encoded))\n",
    "y_train_cnn = to_categorical(y_train_encoded, num_classes)\n",
    "y_val_cnn = to_categorical(y_val_encoded, num_classes)\n",
    "y_test_cnn = to_categorical(y_test_encoded, num_classes)\n"
   ],
   "metadata": {
    "id": "e5d4b20b29631220",
    "outputId": "ea98b3fe-96f7-4173-b586-671ec3a65118",
    "ExecuteTime": {
     "end_time": "2024-06-02T05:08:58.131156Z",
     "start_time": "2024-06-02T05:08:52.134550Z"
    }
   },
   "id": "f96e03aad692f787",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T17:43:09.895293Z",
     "start_time": "2024-06-01T17:42:59.623665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print unique encoded labels to check\n",
    "print(f'Encoded labels: {np.unique(y_train_encoded)}')\n",
    "\n",
    "# Build the 1D CNN model\n",
    "model = Sequential([\n",
    "    Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(64, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the 1D CNN model\n",
    "history = model.fit(X_train_cnn, y_train_cnn, validation_data=(X_val_cnn, y_val_cnn), epochs=15, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_loss, val_accuracy = model.evaluate(X_val_cnn, y_val_cnn)\n",
    "print(f'1D CNN Validation Accuracy: {val_accuracy}')\n"
   ],
   "id": "e5d4b20b29631220",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "id": "63c617a29fafd061"
   },
   "cell_type": "markdown",
   "source": [
    "CNN Model Variant:"
   ],
   "id": "63c617a29fafd061"
  },
  {
   "metadata": {
    "id": "1b5b10515fe79e13",
    "outputId": "2abc8222-2768-4594-b376-5cb6fbfa7b4e",
    "ExecuteTime": {
     "end_time": "2024-06-02T05:56:58.494644Z",
     "start_time": "2024-06-02T05:09:20.084511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the number of labels (classes)\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# Persistent best model - updated by training\n",
    "model_save_path = 'word_model.h5'\n",
    "\n",
    "# Define the modified model\n",
    "model_v2 = Sequential([\n",
    "    # First Conv1D layer\n",
    "    Conv1D(filters=8, kernel_size=13, padding='valid', activation='relu', strides=1, input_shape=(X_train_cnn.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=3),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Second Conv1D layer\n",
    "    Conv1D(filters=16, kernel_size=11, padding='valid', activation='relu', strides=1),\n",
    "    MaxPooling1D(pool_size=3),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Third Conv1D layer\n",
    "    Conv1D(filters=32, kernel_size=9, padding='valid', activation='relu', strides=1),\n",
    "    MaxPooling1D(pool_size=3),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Fourth Conv1D layer\n",
    "    Conv1D(filters=64, kernel_size=7, padding='valid', activation='relu', strides=1),\n",
    "    MaxPooling1D(pool_size=3),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Flatten layer\n",
    "    Flatten(),\n",
    "\n",
    "    # Dense Layer 1\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Dense Layer 2\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Output layer\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_v2.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_v2 = model_v2.fit(X_train_cnn, y_train_cnn, validation_data=(X_val_cnn, y_val_cnn), epochs=12, batch_size=24)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_loss_v2, val_accuracy_v2 = model_v2.evaluate(X_val_cnn, y_val_cnn)\n",
    "print(f'Modified 1D CNN Validation Accuracy: {val_accuracy_v2}')\n",
    "print(f'Modified 1D CNN Validation Loss: {val_loss_v2}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss_v2, test_accuracy_v2 = model_v2.evaluate(X_test_cnn, y_test_cnn)\n",
    "print(f'Modified 1D CNN Test Accuracy: {test_accuracy_v2}')\n",
    "print(f'Modified 1D CNN Test Loss: {test_loss_v2}')\n"
   ],
   "id": "947554cc0f545990",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T06:05:52.562223Z",
     "start_time": "2024-06-02T06:05:20.996024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model_v2.predict(X_val_cnn)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_val_classes = np.argmax(y_val_cnn, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"Conv1D Neural Network Classifier Report\")\n",
    "print(classification_report(y_val_classes, y_pred_classes, zero_division=0))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_val_classes, y_pred_classes)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Learning curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history_v2.history['loss'], label='Training Loss')\n",
    "plt.plot(history_v2.history['val_loss'], label='Validation Loss')\n",
    "plt.plot(history_v2.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_v2.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.axvline(np.argmin(history_v2.history['val_loss']), color='r', linestyle='--', label='Early Stopping Checkpoint')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss/Accuracy')\n",
    "plt.title('Learning Curves')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Load the previous best model and evaluate its performance\n",
    "if os.path.exists(model_save_path):\n",
    "    previous_model = load_model(model_save_path)\n",
    "    previous_val_loss, previous_val_accuracy = previous_model.evaluate(X_val_cnn, y_val_cnn)\n",
    "    print(f\"Previous model validation accuracy: {previous_val_accuracy}\")\n",
    "else:\n",
    "    previous_val_accuracy = 0\n",
    "    print(\"No previous model found. Saving current model as the best model.\")\n",
    "\n",
    "# Compare the performance of the current model with the previous best model\n",
    "current_val_loss, current_val_accuracy = model_v2.evaluate(X_val_cnn, y_val_cnn)\n",
    "print(f\"Current model validation accuracy: {current_val_accuracy}\")\n",
    "\n",
    "if current_val_accuracy > previous_val_accuracy:\n",
    "    print(\"Current model outperforms the previous model. Saving the current model.\")\n",
    "    model_v2.save(model_save_path)\n",
    "else:\n",
    "    print(\"Previous model outperforms the current model. Keeping the previous model.\")\n"
   ],
   "id": "1b5b10515fe79e13",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "id": "75df67294be59488"
   },
   "cell_type": "markdown",
   "source": [
    "With more epochs (20=10+10):"
   ],
   "id": "75df67294be59488"
  },
  {
   "metadata": {
    "id": "3d55247452409832",
    "outputId": "0b0b7256-0ab6-414b-fe1f-2af6cf32d672",
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-06-02T06:07:56.494717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the number of labels (classes)\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# Define the modified model\n",
    "model_v2 = Sequential([\n",
    "    # First Conv1D layer\n",
    "    Conv1D(filters=8, kernel_size=13, padding='valid', activation='relu', strides=1, input_shape=(X_train_cnn.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=3),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Second Conv1D layer\n",
    "    Conv1D(filters=16, kernel_size=11, padding='valid', activation='relu', strides=1),\n",
    "    MaxPooling1D(pool_size=3),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Third Conv1D layer\n",
    "    Conv1D(filters=32, kernel_size=9, padding='valid', activation='relu', strides=1),\n",
    "    MaxPooling1D(pool_size=3),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Fourth Conv1D layer\n",
    "    Conv1D(filters=64, kernel_size=7, padding='valid', activation='relu', strides=1),\n",
    "    MaxPooling1D(pool_size=3),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Flatten layer\n",
    "    Flatten(),\n",
    "\n",
    "    # Dense Layer 1\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Dense Layer 2\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Output layer\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_v2.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_v2 = model_v2.fit(X_train_cnn, y_train_cnn, validation_data=(X_val_cnn, y_val_cnn), epochs=20, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_loss_v2, val_accuracy_v2 = model_v2.evaluate(X_val_cnn, y_val_cnn)\n",
    "print(f'Modified 1D CNN Validation Accuracy: {val_accuracy_v2}')\n",
    "print(f'Modified 1D CNN Validation Loss: {val_loss_v2}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss_v2, test_accuracy_v2 = model_v2.evaluate(X_test_cnn, y_test_cnn)\n",
    "print(f'Modified 1D CNN Test Accuracy: {test_accuracy_v2}')\n",
    "print(f'Modified 1D CNN Test Loss: {test_loss_v2}')\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model_v2.predict(X_val_cnn)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_val_classes = np.argmax(y_val_cnn, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"Conv1D Neural Network Classifier Report\")\n",
    "print(classification_report(y_val_classes, y_pred_classes, zero_division=0))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_val_classes, y_pred_classes)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Learning curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history_v2.history['loss'], label='Training Loss')\n",
    "plt.plot(history_v2.history['val_loss'], label='Validation Loss')\n",
    "plt.plot(history_v2.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_v2.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.axvline(np.argmin(history_v2.history['val_loss']), color='r', linestyle='--', label='Early Stopping Checkpoint')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss/Accuracy')\n",
    "plt.title('Learning Curves')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Load the previous best model and evaluate its performance\n",
    "if os.path.exists(model_save_path):\n",
    "    previous_model = load_model(model_save_path)\n",
    "    previous_val_loss, previous_val_accuracy = previous_model.evaluate(X_val_cnn, y_val_cnn)\n",
    "    print(f\"Previous model validation accuracy: {previous_val_accuracy}\")\n",
    "else:\n",
    "    previous_val_accuracy = 0\n",
    "    print(\"No previous model found. Saving current model as the best model.\")\n",
    "\n",
    "# Compare the performance of the current model with the previous best model\n",
    "current_val_loss, current_val_accuracy = model_v2.evaluate(X_val_cnn, y_val_cnn)\n",
    "print(f\"Current model validation accuracy: {current_val_accuracy}\")\n",
    "\n",
    "if current_val_accuracy > previous_val_accuracy:\n",
    "    print(\"Current model outperforms the previous model. Saving the current model.\")\n",
    "    model_v2.save(model_save_path)\n",
    "else:\n",
    "    print(\"Previous model outperforms the current model. Keeping the previous model.\")\n"
   ],
   "id": "3d55247452409832",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "51ebc515ae260c9a"
   },
   "cell_type": "markdown",
   "source": [
    "With fewer epochs(8=10-2):"
   ],
   "id": "51ebc515ae260c9a"
  },
  {
   "metadata": {
    "id": "df56c6a4d205259a",
    "outputId": "ad3b03ba-f735-4c9c-c653-01d9ccaebac3",
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the number of labels (classes)\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# Define the modified model\n",
    "model_v2 = Sequential([\n",
    "    # First Conv1D layer\n",
    "    Conv1D(filters=8, kernel_size=13, padding='valid', activation='relu', strides=1, input_shape=(X_train_cnn.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=3),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Second Conv1D layer\n",
    "    Conv1D(filters=16, kernel_size=11, padding='valid', activation='relu', strides=1),\n",
    "    MaxPooling1D(pool_size=3),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Third Conv1D layer\n",
    "    Conv1D(filters=32, kernel_size=9, padding='valid', activation='relu', strides=1),\n",
    "    MaxPooling1D(pool_size=3),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Fourth Conv1D layer\n",
    "    Conv1D(filters=64, kernel_size=7, padding='valid', activation='relu', strides=1),\n",
    "    MaxPooling1D(pool_size=3),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Flatten layer\n",
    "    Flatten(),\n",
    "\n",
    "    # Dense Layer 1\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Dense Layer 2\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Output layer\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_v2.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_v2 = model_v2.fit(X_train_cnn, y_train_cnn, validation_data=(X_val_cnn, y_val_cnn), epochs=8, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_loss_v2, val_accuracy_v2 = model_v2.evaluate(X_val_cnn, y_val_cnn)\n",
    "print(f'Modified 1D CNN Validation Accuracy: {val_accuracy_v2}')\n",
    "print(f'Modified 1D CNN Validation Loss: {val_loss_v2}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss_v2, test_accuracy_v2 = model_v2.evaluate(X_test_cnn, y_test_cnn)\n",
    "print(f'Modified 1D CNN Test Accuracy: {test_accuracy_v2}')\n",
    "print(f'Modified 1D CNN Test Loss: {test_loss_v2}')\n",
    "\n",
    "# Load the previous best model and evaluate its performance\n",
    "if os.path.exists(model_save_path):\n",
    "    previous_model = load_model(model_save_path)\n",
    "    previous_val_loss, previous_val_accuracy = previous_model.evaluate(X_val_cnn, y_val_cnn)\n",
    "    print(f\"Previous model validation accuracy: {previous_val_accuracy}\")\n",
    "else:\n",
    "    previous_val_accuracy = 0\n",
    "    print(\"No previous model found. Saving current model as the best model.\")\n",
    "\n",
    "# Compare the performance of the current model with the previous best model\n",
    "current_val_loss, current_val_accuracy = model_v2.evaluate(X_val_cnn, y_val_cnn)\n",
    "print(f\"Current model validation accuracy: {current_val_accuracy}\")\n",
    "\n",
    "if current_val_accuracy > previous_val_accuracy:\n",
    "    print(\"Current model outperforms the previous model. Saving the current model.\")\n",
    "    model_v2.save(model_save_path)\n",
    "else:\n",
    "    print(\"Previous model outperforms the current model. Keeping the previous model.\")\n"
   ],
   "id": "df56c6a4d205259a",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Assuming the data is already loaded into X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Reshape data for 1D CNN\n",
    "X_train_cnn = X_train.reshape(-1, X_train.shape[1], 1)\n",
    "X_val_cnn = X_val.reshape(-1, X_val.shape[1], 1)\n",
    "X_test_cnn = X_test.reshape(-1, X_test.shape[1], 1)\n",
    "\n",
    "# Encode labels as numeric\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Save the class names\n",
    "word_class_names = list(label_encoder.classes_)\n",
    "with open(f'{data_dir}/word_class_names.json', 'w') as f:\n",
    "    json.dump(word_class_names, f)\n",
    "\n",
    "# Convert labels to categorical\n",
    "num_classes = len(np.unique(y_train_encoded))\n",
    "y_train_cnn = to_categorical(y_train_encoded, num_classes)\n",
    "y_val_cnn = to_categorical(y_val_encoded, num_classes)\n",
    "y_test_cnn = to_categorical(y_test_encoded, num_classes)\n",
    "\n",
    "# Define the modified model\n",
    "model_v2 = Sequential([\n",
    "    # First Conv1D layer\n",
    "    Conv1D(filters=8, kernel_size=13, padding='valid', activation='relu', strides=1, input_shape=(X_train_cnn.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=3),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Second Conv1D layer\n",
    "    Conv1D(filters=16, kernel_size=11, padding='valid', activation='relu', strides=1),\n",
    "    MaxPooling1D(pool_size=3),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Third Conv1D layer\n",
    "    Conv1D(filters=32, kernel_size=9, padding='valid', activation='relu', strides=1),\n",
    "    MaxPooling1D(pool_size=3),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Fourth Conv1D layer\n",
    "    Conv1D(filters=64, kernel_size=7, padding='valid', activation='relu', strides=1),\n",
    "    MaxPooling1D(pool_size=3),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Flatten layer\n",
    "    Flatten(),\n",
    "\n",
    "    # Dense Layer 1\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Dense Layer 2\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Output layer\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_v2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_v2 = model_v2.fit(X_train_cnn, y_train_cnn, validation_data=(X_val_cnn, y_val_cnn), epochs=20, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_loss_v2, val_accuracy_v2 = model_v2.evaluate(X_val_cnn, y_val_cnn)\n",
    "print(f'Modified 1D CNN Validation Accuracy: {val_accuracy_v2}')\n",
    "print(f'Modified 1D CNN Validation Loss: {val_loss_v2}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss_v2, test_accuracy_v2 = model_v2.evaluate(X_test_cnn, y_test_cnn)\n",
    "print(f'Modified 1D CNN Test Accuracy: {test_accuracy_v2}')\n",
    "print(f'Modified 1D CNN Test Loss: {test_loss_v2}')\n",
    "\n",
    "# Save the model\n",
    "model_save_path = f'{data_dir}/word_model.h5'\n",
    "model_v2.save(model_save_path)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model_v2.predict(X_val_cnn)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_val_classes = np.argmax(y_val_cnn, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"Conv1D Neural Network Classifier Report\")\n",
    "print(classification_report(y_val_classes, y_pred_classes, zero_division=0))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_val_classes, y_pred_classes)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Learning curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history_v2.history['loss'], label='Training Loss')\n",
    "plt.plot(history_v2.history['val_loss'], label='Validation Loss')\n",
    "plt.plot(history_v2.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_v2.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.axvline(np.argmin(history_v2.history['val_loss']), color='r', linestyle='--', label='Early Stopping Checkpoint')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss/Accuracy')\n",
    "plt.title('Learning Curves')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Load the previous best model and evaluate its performance\n",
    "if os.path.exists(model_save_path):\n",
    "    previous_model = load_model(model_save_path)\n",
    "    previous_val_loss, previous_val_accuracy = previous_model.evaluate(X_val_cnn, y_val_cnn)\n",
    "    print(f\"Previous model validation accuracy: {previous_val_accuracy}\")\n",
    "else:\n",
    "    previous_val_accuracy = 0\n",
    "    print(\"No previous model found. Saving current model as the best model.\")\n",
    "\n",
    "# Compare the performance of the current model with the previous best model\n",
    "current_val_loss, current_val_accuracy = model_v2.evaluate(X_val_cnn, y_val_cnn)\n",
    "print(f\"Current model validation accuracy: {current_val_accuracy}\")\n",
    "\n",
    "if current_val_accuracy > previous_val_accuracy:\n",
    "    print(\"Current model outperforms the previous model. Saving the current model.\")\n",
    "    model_v2.save(model_save_path)\n",
    "else:\n",
    "    print(\"Previous model outperforms the current model. Keeping the previous model.\")\n"
   ],
   "id": "57471ca6468adacf",
   "outputs": []
  },
  {
   "metadata": {
    "id": "3b8f4769cd3d964c"
   },
   "cell_type": "markdown",
   "source": [
    "Metrics:"
   ],
   "id": "3b8f4769cd3d964c"
  },
  {
   "metadata": {
    "id": "fab04e7271b969d1",
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_val = model_v2.predict(X_val_cnn)\n",
    "y_pred_val_classes = y_pred_val.argmax(axis=1)  # Convert probabilities to class labels\n",
    "\n",
    "# Convert y_val_cnn to class labels if needed\n",
    "y_val_classes = y_val_cnn.argmax(axis=1)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy_val = accuracy_score(y_val_classes, y_pred_val_classes)\n",
    "precision_val = precision_score(y_val_classes, y_pred_val_classes, average='weighted')\n",
    "recall_val = recall_score(y_val_classes, y_pred_val_classes, average='weighted')\n",
    "f1_val = f1_score(y_val_classes, y_pred_val_classes, average='weighted')\n",
    "\n",
    "print(f'CNN Validation Accuracy: {accuracy_val}')\n",
    "print(f'CNN Validation Precision: {precision_val}')\n",
    "print(f'CNN Validation Recall: {recall_val}')\n",
    "print(f'CNN Validation F1-Score: {f1_val}')"
   ],
   "id": "fab04e7271b969d1",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "8ed1bbabb110b55b"
   },
   "cell_type": "markdown",
   "source": [
    "### Test Set"
   ],
   "id": "8ed1bbabb110b55b"
  },
  {
   "cell_type": "code",
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model_v2.evaluate(X_test_cnn, y_test_cnn)\n",
    "print(f'1D CNN Test Accuracy: {test_accuracy}')\n",
    "print(f'1D CNN Test Loss: {test_loss}')\n"
   ],
   "metadata": {
    "id": "e8058964645f0249",
    "outputId": "e70e2be1-9f29-4b7b-9e3e-54d09fe6bdf8",
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "e8058964645f0249",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "4b4fdbd8a669efe1",
    "outputId": "8909a61f-3ab3-49e0-d184-06f416a58249",
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_test = model_v2.predict(X_test_cnn).argmax(axis=1)\n",
    "y_true_test = y_test_cnn.argmax(axis=1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true_test, y_pred_test)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ],
   "id": "4b4fdbd8a669efe1",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "c30ea9dc005ba23d",
    "outputId": "5e5aa313-83d4-45d9-815d-433a98b29dad",
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history_v2.history['accuracy'])\n",
    "plt.plot(history_v2.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history_v2.history['loss'])\n",
    "plt.plot(history_v2.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n"
   ],
   "id": "c30ea9dc005ba23d",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "de070ce0b6c1298e"
   },
   "cell_type": "markdown",
   "source": [
    "### Validation Set"
   ],
   "id": "de070ce0b6c1298e"
  },
  {
   "metadata": {
    "id": "8216105e59707aca",
    "outputId": "7a98adc0-b7f6-42e1-ffa2-f6b6a5e8081a",
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_val = model_v2.predict(X_val_cnn).argmax(axis=1)\n",
    "y_true_val = y_val_cnn.argmax(axis=1)\n",
    "\n",
    "# Fake the result - just in case - on the training set\n",
    "y_pred_val = model_v2.predict(X_val_cnn).argmax(axis=1)\n",
    "y_true_val = y_val_cnn.argmax(axis=1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true_val, y_pred_val)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ],
   "id": "8216105e59707aca",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "f6fd771a8f0738f0",
    "outputId": "ec89ec77-b023-4e0a-b33f-fbf23884033f",
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_true_val, y_pred_val, target_names=label_encoder.classes_)\n",
    "print(report)\n"
   ],
   "id": "f6fd771a8f0738f0",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "a63918d17b5f554b",
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [],
   "id": "a63918d17b5f554b",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "ef4519360bdc5696"
   },
   "cell_type": "markdown",
   "source": [
    "Confusion matrix and learning curves again:"
   ],
   "id": "ef4519360bdc5696"
  },
  {
   "metadata": {
    "id": "1142947dc35c71af",
    "outputId": "03f88b87-90d6-430b-c765-aa0e327cb8da",
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# Ensure the 'fig' directory exists\n",
    "os.makedirs('fig', exist_ok=True)\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_test = model_v2.predict(X_test_cnn).argmax(axis=1)\n",
    "y_true_test = y_test_cnn.argmax(axis=1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true_test, y_pred_test)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('fig/confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Learning Curves\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history_v2.history['accuracy'])\n",
    "plt.plot(history_v2.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.savefig('fig/learning_curves_accuracy.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history_v2.history['loss'])\n",
    "plt.plot(history_v2.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.savefig('fig/learning_curves_loss.png')\n",
    "plt.show()\n"
   ],
   "id": "1142947dc35c71af",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "bc84b510b5998ea2",
    "ExecuteTime": {
     "end_time": "2024-06-02T11:26:57.794786Z",
     "start_time": "2024-06-02T11:04:03.717283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Function to load preprocessed data\n",
    "def load_preprocessed_data(preprocessed_dir):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for file_name in os.listdir(preprocessed_dir):\n",
    "        if file_name.startswith('audio'):\n",
    "            audio = np.load(os.path.join(preprocessed_dir, file_name))\n",
    "            label_file = file_name.replace('audio', 'label')\n",
    "            label = np.load(os.path.join(preprocessed_dir, label_file))\n",
    "\n",
    "            X.append(audio)\n",
    "            y.append(label)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load preprocessed data\n",
    "preprocessed_dir = 'preprocessed_data'\n",
    "X, y = load_preprocessed_data(preprocessed_dir)\n",
    "\n",
    "print(f'Loaded {len(X)} preprocessed audio files.')\n",
    "print(f'Labels: {set(y)}')\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f'Training set size: {len(X_train)}')\n",
    "print(f'Validation set size: {len(X_val)}')\n",
    "print(f'Test set size: {len(X_test)}')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "data_dir = '../dataset'\n",
    "# Assuming the data is already loaded into X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Reshape data for 1D CNN\n",
    "X_train_cnn = X_train.reshape(-1, X_train.shape[1], 1)\n",
    "X_val_cnn = X_val.reshape(-1, X_val.shape[1], 1)\n",
    "X_test_cnn = X_test.reshape(-1, X_test.shape[1], 1)\n",
    "\n",
    "# Encode labels as numeric\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Save the class names\n",
    "word_class_names = list(label_encoder.classes_)\n",
    "with open(f'{data_dir}/word_class_names.json', 'w') as f:\n",
    "    json.dump(word_class_names, f)\n",
    "\n",
    "# Convert labels to categorical\n",
    "num_classes = len(np.unique(y_train_encoded))\n",
    "y_train_cnn = to_categorical(y_train_encoded, num_classes)\n",
    "y_val_cnn = to_categorical(y_val_encoded, num_classes)\n",
    "y_test_cnn = to_categorical(y_test_encoded, num_classes)\n",
    "\n",
    "# Define the modified model\n",
    "model_v2 = Sequential([\n",
    "    # First Conv1D layer\n",
    "    Conv1D(filters=8, kernel_size=13, padding='valid', activation='relu', strides=1, input_shape=(X_train_cnn.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=3),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Second Conv1D layer\n",
    "    Conv1D(filters=16, kernel_size=11, padding='valid', activation='relu', strides=1),\n",
    "    MaxPooling1D(pool_size=3),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Third Conv1D layer\n",
    "    Conv1D(filters=32, kernel_size=9, padding='valid', activation='relu', strides=1),\n",
    "    MaxPooling1D(pool_size=3),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Fourth Conv1D layer\n",
    "    Conv1D(filters=64, kernel_size=7, padding='valid', activation='relu', strides=1),\n",
    "    MaxPooling1D(pool_size=3),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Flatten layer\n",
    "    Flatten(),\n",
    "\n",
    "    # Dense Layer 1\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Dense Layer 2\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Output layer\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_v2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_v2 = model_v2.fit(X_train_cnn, y_train_cnn, validation_data=(X_val_cnn, y_val_cnn), epochs=20, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_loss_v2, val_accuracy_v2 = model_v2.evaluate(X_val_cnn, y_val_cnn)\n",
    "print(f'Modified 1D CNN Validation Accuracy: {val_accuracy_v2}')\n",
    "print(f'Modified 1D CNN Validation Loss: {val_loss_v2}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss_v2, test_accuracy_v2 = model_v2.evaluate(X_test_cnn, y_test_cnn)\n",
    "print(f'Modified 1D CNN Test Accuracy: {test_accuracy_v2}')\n",
    "print(f'Modified 1D CNN Test Loss: {test_loss_v2}')\n",
    "\n",
    "# Save the model\n",
    "model_save_path = f'{data_dir}/word_model.h5'\n",
    "model_v2.save(model_save_path)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model_v2.predict(X_val_cnn)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_val_classes = np.argmax(y_val_cnn, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"Conv1D Neural Network Classifier Report\")\n",
    "print(classification_report(y_val_classes, y_pred_classes, zero_division=0))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_val_classes, y_pred_classes)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Learning curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history_v2.history['loss'], label='Training Loss')\n",
    "plt.plot(history_v2.history['val_loss'], label='Validation Loss')\n",
    "plt.plot(history_v2.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_v2.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.axvline(np.argmin(history_v2.history['val_loss']), color='r', linestyle='--', label='Early Stopping Checkpoint')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss/Accuracy')\n",
    "plt.title('Learning Curves')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Load the previous best model and evaluate its performance\n",
    "if os.path.exists(model_save_path):\n",
    "    previous_model = load_model(model_save_path)\n",
    "    previous_val_loss, previous_val_accuracy = previous_model.evaluate(X_val_cnn, y_val_cnn)\n",
    "    print(f\"Previous model validation accuracy: {previous_val_accuracy}\")\n",
    "else:\n",
    "    previous_val_accuracy = 0\n",
    "    print(\"No previous model found. Saving current model as the best model.\")\n",
    "\n",
    "# Compare the performance of the current model with the previous best model\n",
    "current_val_loss, current_val_accuracy = model_v2.evaluate(X_val_cnn, y_val_cnn)\n",
    "print(f\"Current model validation accuracy: {current_val_accuracy}\")\n",
    "\n",
    "if current_val_accuracy > previous_val_accuracy:\n",
    "    print(\"Current model outperforms the previous model. Saving the current model.\")\n",
    "    model_v2.save(model_save_path)\n",
    "else:\n",
    "    print(\"Previous model outperforms the current model. Keeping the previous model.\")\n"
   ],
   "id": "bc84b510b5998ea2",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "a583f02282b2de13",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
