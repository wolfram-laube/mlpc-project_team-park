\section{Experiments}

\subsection{Random Forest}
\subsubsection{Classification Performance with Varying Hyperparameters}
The key hyperparameters evaluated were the number of estimators and the maximum depth. The performance impact of varying these two parameters were primarily analyzed. The graph demonstrates that increasing both parameters leads to significant accuracy improvements up to a certain point, beyond which further increases result in diminishing returns.
\begin{figure}[!ht]
	\centering
    \includegraphics[scale=0.5]{fig/rfc_parameters.png}
	\vspace{-0.3cm}
	\caption{Performance on different hyperparameters}
	\label{fig:rfc_parameters}
	\vspace{-0.1cm}
\end{figure}
\subsubsection{Overfitting and Underfitting Analysis}
Overfitting was particularly evident when using high numbers of estimators, but this was mitigated by visualizing the accuracy across different numbers of estimators and adjusting the parameter accordingly based on the plot. 
The training score remained consistently at 1.0 or 100\% across all sample sizes, indicating that the model perfectly fit the training data. This perfect score, however, suggests that the model was likely overfitting. In contrast, the cross-validation score started at approximately 75\% with smaller sample sizes and gradually increased to about 90\% as the sample size grew. This improvement indicates that the model's ability to generalize to unseen data improved with more training samples, thereby reducing overfitting.

\begin{figure}[!ht]
	\centering
	\includegraphics[scale=0.5]{fig/rfc_learning_curve.png}
	\vspace{-0.3cm}
	\caption{Learning Curve}
	\label{fig:rfc_learning_curve}
	\vspace{-0.1cm}
\end{figure}
\subsubsection{Final Unbiased Performance Comparison}
Overall, the Random Forest model demonstrates robust performance in classifying the speech data, with particularly high accuracy and well-balanced precision and recall across most categories. The exceptions that did not perform as well were expected, as words like "Ofen," "offen," "Licht," and "nicht" are pronounced quite similarly.

\begin{table}
  \caption{Classification Report for Random Forest Model}
\label{tab:classification_report}
  \centering
  \begin{tabular}{lccc}
    \toprule
    Class        & Precision & Recall & F1-score \\
    \midrule
    Alarm        & 0.93 & 0.91 & 0.92 \\
    Command an   & 0.91 & 0.87 & 0.89 \\
    Command aus  & 0.89 & 0.86 & 0.87 \\
    Fenseher     & 0.98 & 0.88 & 0.93 \\
    Heizung      & 0.99 & 0.89 & 0.94 \\
    Licht        & 0.93 & 0.82 & 0.87 \\
    LÃ¼ftung      & 0.98 & 0.93 & 0.95 \\
    Miscellaneous & 0.89 & 0.96 & 0.92 \\
    Objects      & 0.90 & 0.96 & 0.93 \\
    Ofen         & 0.82 & 0.70 & 0.76 \\
    Radio        & 0.99 & 0.88 & 0.93 \\
    Status       & 0.84 & 0.86 & 0.85 \\
    Staubsauger  & 0.99 & 0.90 & 0.94 \\
    \midrule
    Accuracy     & \multicolumn{3}{c}{0.91} \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Nearest Neighbour}
\subsubsection{Classification Performance with Varying Hyperparameters}
Present the results of experiments with different hyperparameter values. Visualize the change in performance.
\subsubsection{Overfitting and Underfitting Analysis}
Discuss the extent of overfitting or underfitting observed. Explain how these issues depend on hyperparameter values.
\subsubsection{Final Unbiased Performance Comparison}
Summarize the results in a comparative table or plot.

\subsection{CNN}
\subsubsection{Classification Performance with Varying Hyperparameters}
Present the results of experiments with different hyperparameter values. Visualize the change in performance.
\subsubsection{Overfitting and Underfitting Analysis}
Discuss the extent of overfitting or underfitting observed. Explain how these issues depend on hyperparameter values.
\subsubsection{Final Unbiased Performance Comparison}
Summarize the results in a comparative table or plot.
