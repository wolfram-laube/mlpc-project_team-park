\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023

% ready for submission
\usepackage[final]{neurips_2023}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{graphicx}       % colors

\title{MLPC Report Task 3 - Classification Experiments}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Team Park \And
  Author Oliver König \And
  Author Daniel Hörtenhuber \And
  Author Wolfram Laube
}

\begin{document}

\maketitle

\begin{abstract}
This report details the experiments conducted for speech command recognition using various classifiers. Initially, three classifiers were considered: Random Forest, Convolutional Neural Network (CNN), and K-Nearest Neighbour (KNN). As the experiments progressed, it became evident that KNN would not be a serious contender compared to Random Forest and CNN due to its lower performance. Consequently, the focus was shifted entirely to evaluating Random Forest and CNN. This report covers the methodologies, preprocessing steps, and performance evaluations of the selected classifiers, leading to the conclusion that the CNN model demonstrated superior performance.
\end{abstract}

\section{Introduction}
Speech command recognition is a crucial aspect of human-computer interaction, enabling users to control systems through voice commands. This project aims to evaluate different classifiers for speech command recognition using a dataset of audio recordings. Initially, we considered three classifiers: Random Forest, Convolutional Neural Network (CNN), and K-Nearest Neighbour (KNN).

As the level of concretization increased during our experiments, it became clear that KNN was not a serious contender against Random Forest and CNN. The performance gains achieved by Random Forest and CNN were significantly higher and more promising. Given the time constraints and the need to provide a meaningful comparison, we decided to eliminate the KNN classifier from further evaluation. Continuing to evaluate and document a classifier with relatively lower performance was deemed inefficient and would not contribute significantly to the overall findings. Thus, the focus of this report is on the Random Forest and CNN classifiers.

\begin{contributions}
  Section 1: Data Split: Wolfram Laube \AND
  Section 2: Classes & Features: Daniel Hörtenhuber \AND
  Section 3: Evaluation: Oliver König \AND
  Section 4: Experiments: Each team member on their classifier \AND
  Section 5: Analysis of Realistic Scenes: Collaborative
\end{contributions}

\input{report_section1_data_split}
\input{report_section2_classes_features}
\input{report_section3_evaluation}
\input{report_section4_experiments}
\input{report_section5_realistic_scenes}
\input{report_section6_conclusion}

\end{document}
