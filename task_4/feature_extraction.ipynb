{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T06:28:58.426661Z",
     "start_time": "2024-06-04T06:27:40.653471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "data_dir = '../dataset'\n",
    "annotations_file = f'{data_dir}/development_scene_annotations.csv'\n",
    "\n",
    "# Load annotations\n",
    "annotations = pd.read_csv(annotations_file)\n",
    "\n",
    "def extract_features(y, sr, n_mels=128):\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "    log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    return log_mel_spec.T\n",
    "\n",
    "def pad_features(features, max_len):\n",
    "    padded_features = []\n",
    "    for feature in features:\n",
    "        pad_width = max_len - feature.shape[0]\n",
    "        if pad_width > 0:\n",
    "            feature = np.pad(feature, ((0, pad_width), (0, 0)), mode='constant')\n",
    "        padded_features.append(feature)\n",
    "    return np.array(padded_features)\n",
    "\n",
    "def prepare_command_data(annotations, data_dir, sr=16000, n_mels=128):\n",
    "    command_features = []\n",
    "    command_labels = []\n",
    "    command_mapping = {}  # Mapping of command texts to numerical labels\n",
    "    current_label = 0\n",
    "    max_len = 0  # To determine the maximum length of features\n",
    "\n",
    "    for index, row in annotations.iterrows():\n",
    "        audio_path = os.path.join(data_dir, 'scenes', 'wav', row['filename'] + '.wav')\n",
    "        y, _ = librosa.load(audio_path, sr=sr)\n",
    "        start_sample = int(row['start'] * sr)\n",
    "        end_sample = int(row['end'] * sr)\n",
    "        \n",
    "        command_text = row['command']  # Assuming the command text is in this column\n",
    "        if command_text not in command_mapping:\n",
    "            command_mapping[command_text] = current_label\n",
    "            current_label += 1\n",
    "        \n",
    "        command_label = command_mapping[command_text]\n",
    "        command_segment = y[start_sample:end_sample]\n",
    "        features = extract_features(command_segment, sr, n_mels)\n",
    "        max_len = max(max_len, features.shape[0])  # Update max_len\n",
    "        \n",
    "        command_features.append(features)\n",
    "        command_labels.append(command_label)\n",
    "\n",
    "    # Pad features to the same length\n",
    "    command_features = pad_features(command_features, max_len)\n",
    "\n",
    "    return np.array(command_features), np.array(command_labels), command_mapping\n",
    "\n",
    "def create_boundary_detection_data(annotations, data_dir, window_size=0.5, step_size=0.1, sr=16000, n_mels=128):\n",
    "    windows = []\n",
    "    labels = []\n",
    "    for index, row in annotations.iterrows():\n",
    "        audio_path = os.path.join(data_dir, 'scenes', 'wav', row['filename'] + '.wav')\n",
    "        y, _ = librosa.load(audio_path, sr=sr)\n",
    "        start_sample = int(row['start'] * sr)\n",
    "        end_sample = int(row['end'] * sr)\n",
    "\n",
    "        for i in range(start_sample, end_sample - int(window_size * sr), int(step_size * sr)):\n",
    "            window = y[i:i + int(window_size * sr)]\n",
    "            features = extract_features(window, sr, n_mels)\n",
    "            label = 1 if (i == start_sample or i + int(window_size * sr) >= end_sample) else 0\n",
    "            windows.append(features)\n",
    "            labels.append(label)\n",
    "\n",
    "    return np.array(windows), np.array(labels)\n",
    "\n",
    "def build_boundary_detection_model(input_shape):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_command_recognition_model(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def detect_command_pairs(audio_path, boundary_model, command_model, sr=16000, window_size=0.5, step_size=0.1, n_mels=128):\n",
    "    y, _ = librosa.load(audio_path, sr=sr)\n",
    "    window_samples = int(window_size * sr)\n",
    "    step_samples = int(step_size * sr)\n",
    "\n",
    "    windows = librosa.util.frame(y, frame_length=window_samples, hop_length=step_samples)\n",
    "    windows = windows.T.reshape((-1, window_samples))\n",
    "\n",
    "    boundaries = []\n",
    "    for window in windows:\n",
    "        features = extract_features(window, sr, n_mels).reshape((1, n_mels, -1, 1))\n",
    "        boundary_prediction = boundary_model.predict(features)\n",
    "        boundaries.append(boundary_prediction)\n",
    "\n",
    "    boundaries = np.where(np.array(boundaries) > 0.5)[0] * step_samples\n",
    "\n",
    "    command_segments = []\n",
    "    for start in boundaries:\n",
    "        end = start + window_samples\n",
    "        segment = y[start:end]\n",
    "        segment_features = extract_features(segment, sr, n_mels).reshape((1, n_mels, -1, 1))\n",
    "        command_prediction = command_model.predict(segment_features)\n",
    "        command_segments.append((segment, command_prediction))\n",
    "\n",
    "    return command_segments\n",
    "\n",
    "# Prepare command features and labels\n",
    "command_features, command_labels, command_mapping = prepare_command_data(annotations, data_dir)\n",
    "\n",
    "# Reshape features for the CNN\n",
    "command_features = command_features.reshape((command_features.shape[0], command_features.shape[1], command_features.shape[2], 1))\n",
    "\n",
    "# Normalize features\n",
    "command_features = command_features / np.max(command_features)\n",
    "\n",
    "# One-hot encode labels\n",
    "num_classes = len(command_mapping)\n",
    "command_labels = to_categorical(command_labels, num_classes=num_classes)\n",
    "\n",
    "print(command_mapping)\n",
    "\n",
    "# Prepare data for boundary detection\n",
    "windows, labels = create_boundary_detection_data(annotations, data_dir)\n",
    "\n",
    "# Reshape data for the CNN\n",
    "windows = windows.reshape((windows.shape[0], windows.shape[1], windows.shape[2], 1))\n",
    "\n",
    "# Build and train the boundary detection model\n",
    "input_shape = (windows.shape[1], windows.shape[2], 1)\n",
    "boundary_model = build_boundary_detection_model(input_shape)\n",
    "boundary_model.fit(windows, labels, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Build and train the command recognition model\n",
    "input_shape = (command_features.shape[1], command_features.shape[2], 1)\n",
    "command_model = build_command_recognition_model(input_shape, num_classes)\n",
    "command_model.fit(command_features, command_labels, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Detect commands in a new audio file\n",
    "new_audio_path = f'{data_dir}/scenes/wav/2015_speech_true_Ofen_aus_Alarm_an.wav'\n",
    "detected_commands = detect_command_pairs(new_audio_path, boundary_model, command_model)\n",
    "\n",
    "# Print recognized commands\n",
    "for segment, command in detected_commands:\n",
    "    print(command)\n"
   ],
   "id": "f8b094ed7c9a39e0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 08:27:44.690828: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=800\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Licht aus': 0, 'Ofen an': 1, 'Radio an': 2, 'Fernseher an': 3, 'Heizung aus': 4, 'Alarm an': 5, 'Lüftung aus': 6, 'Staubsauger aus': 7, 'Heizung an': 8, 'Staubsauger an': 9, 'Alarm aus': 10, 'Licht an': 11, 'Ofen aus': 12, 'Radio aus': 13, 'Lüftung an': 14, 'Fernseher aus': 15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Conv2D.call().\n\n\u001B[1mNegative dimension size caused by subtracting 3 from 2 for '{{node sequential_1/conv2d_2_1/convolution}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](sequential_1/max_pooling2d_1_2/MaxPool2d, sequential_1/conv2d_2_1/convolution/ReadVariableOp)' with input shapes: [?,2,30,64], [3,3,64,128].\u001B[0m\n\nArguments received by Conv2D.call():\n  • inputs=tf.Tensor(shape=(None, 2, 30, 64), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 158\u001B[0m\n\u001B[1;32m    156\u001B[0m input_shape \u001B[38;5;241m=\u001B[39m (windows\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m], windows\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m], \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    157\u001B[0m boundary_model \u001B[38;5;241m=\u001B[39m build_boundary_detection_model(input_shape)\n\u001B[0;32m--> 158\u001B[0m \u001B[43mboundary_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwindows\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_split\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[38;5;66;03m# Build and train the command recognition model\u001B[39;00m\n\u001B[1;32m    161\u001B[0m input_shape \u001B[38;5;241m=\u001B[39m (command_features\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m], command_features\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m], \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m    120\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m--> 122\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m    120\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m--> 122\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "\u001B[0;31mValueError\u001B[0m: Exception encountered when calling Conv2D.call().\n\n\u001B[1mNegative dimension size caused by subtracting 3 from 2 for '{{node sequential_1/conv2d_2_1/convolution}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](sequential_1/max_pooling2d_1_2/MaxPool2d, sequential_1/conv2d_2_1/convolution/ReadVariableOp)' with input shapes: [?,2,30,64], [3,3,64,128].\u001B[0m\n\nArguments received by Conv2D.call():\n  • inputs=tf.Tensor(shape=(None, 2, 30, 64), dtype=float32)"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1f3701f9d756363"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
