{
 "cells": [
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-06-04T06:41:45.723249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "data_dir = '../dataset'\n",
    "annotations_file = f'{data_dir}/development_scene_annotations.csv'\n",
    "\n",
    "# Load annotations\n",
    "logging.info('Loading annotations...')\n",
    "annotations = pd.read_csv(annotations_file)\n",
    "logging.info('Annotations loaded.')\n",
    "\n",
    "def extract_features(y, sr, n_mels=128):\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "    log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    return log_mel_spec.T\n",
    "\n",
    "def pad_features(features, max_len):\n",
    "    padded_features = []\n",
    "    for feature in features:\n",
    "        pad_width = max_len - feature.shape[0]\n",
    "        if pad_width > 0:\n",
    "            feature = np.pad(feature, ((0, pad_width), (0, 0)), mode='constant')\n",
    "        padded_features.append(feature)\n",
    "    return np.array(padded_features)\n",
    "\n",
    "def prepare_command_data(annotations, data_dir, sr=16000, n_mels=128):\n",
    "    command_features = []\n",
    "    command_labels = []\n",
    "    command_mapping = {}  # Mapping of command texts to numerical labels\n",
    "    current_label = 0\n",
    "    max_len = 0  # To determine the maximum length of features\n",
    "\n",
    "    logging.info('Preparing command data...')\n",
    "    for index, row in tqdm(annotations.iterrows(), total=annotations.shape[0]):\n",
    "        audio_path = os.path.join(data_dir, 'scenes', 'wav', row['filename'] + '.wav')\n",
    "        y, _ = librosa.load(audio_path, sr=sr)\n",
    "        start_sample = int(row['start'] * sr)\n",
    "        end_sample = int(row['end'] * sr)\n",
    "        \n",
    "        command_text = row['command']  # Assuming the command text is in this column\n",
    "        if command_text not in command_mapping:\n",
    "            command_mapping[command_text] = current_label\n",
    "            current_label += 1\n",
    "        \n",
    "        command_label = command_mapping[command_text]\n",
    "        command_segment = y[start_sample:end_sample]\n",
    "        features = extract_features(command_segment, sr, n_mels)\n",
    "        max_len = max(max_len, features.shape[0])  # Update max_len\n",
    "        \n",
    "        command_features.append(features)\n",
    "        command_labels.append(command_label)\n",
    "\n",
    "    # Pad features to the same length\n",
    "    command_features = pad_features(command_features, max_len)\n",
    "    logging.info('Command data prepared.')\n",
    "\n",
    "    return np.array(command_features), np.array(command_labels), command_mapping\n",
    "\n",
    "def create_boundary_detection_data(annotations, data_dir, window_size=0.5, step_size=0.1, sr=16000, n_mels=128):\n",
    "    windows = []\n",
    "    labels = []\n",
    "    logging.info('Creating boundary detection data...')\n",
    "    for index, row in tqdm(annotations.iterrows(), total=annotations.shape[0]):\n",
    "        audio_path = os.path.join(data_dir, 'scenes', 'wav', row['filename'] + '.wav')\n",
    "        y, _ = librosa.load(audio_path, sr=sr)\n",
    "        start_sample = int(row['start'] * sr)\n",
    "        end_sample = int(row['end'] * sr)\n",
    "\n",
    "        for i in range(start_sample, end_sample - int(window_size * sr), int(step_size * sr)):\n",
    "            window = y[i:i + int(window_size * sr)]\n",
    "            features = extract_features(window, sr, n_mels)\n",
    "            label = 1 if (i == start_sample or i + int(window_size * sr) >= end_sample) else 0\n",
    "            windows.append(features)\n",
    "            labels.append(label)\n",
    "    \n",
    "    logging.info('Boundary detection data created.')\n",
    "    return np.array(windows), np.array(labels)\n",
    "\n",
    "def build_boundary_detection_model(input_shape):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_command_recognition_model(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def detect_command_pairs(audio_path, boundary_model, command_model, sr=16000, window_size=0.5, step_size=0.1, n_mels=128):\n",
    "    y, _ = librosa.load(audio_path, sr=sr)\n",
    "    window_samples = int(window_size * sr)\n",
    "    step_samples = int(step_size * sr)\n",
    "\n",
    "    windows = librosa.util.frame(y, frame_length=window_samples, hop_length=step_samples)\n",
    "    windows = windows.T.reshape((-1, window_samples))\n",
    "\n",
    "    boundaries = []\n",
    "    for window in windows:\n",
    "        features = extract_features(window, sr, n_mels).reshape((1, n_mels, -1, 1))\n",
    "        boundary_prediction = boundary_model.predict(features)\n",
    "        boundaries.append(boundary_prediction)\n",
    "\n",
    "    boundaries = np.where(np.array(boundaries) > 0.5)[0] * step_samples\n",
    "\n",
    "    command_segments = []\n",
    "    for start in boundaries:\n",
    "        end = start + window_samples\n",
    "        segment = y[start:end]\n",
    "        segment_features = extract_features(segment, sr, n_mels).reshape((1, n_mels, -1, 1))\n",
    "        command_prediction = command_model.predict(segment_features)\n",
    "        command_segments.append((segment, command_prediction))\n",
    "\n",
    "    return command_segments\n",
    "\n",
    "# Prepare command features and labels\n",
    "command_features, command_labels, command_mapping = prepare_command_data(annotations, data_dir)\n",
    "\n",
    "# Reshape features for the CNN\n",
    "command_features = command_features.reshape((command_features.shape[0], command_features.shape[1], command_features.shape[2], 1))\n",
    "\n",
    "# Normalize features\n",
    "command_features = command_features / np.max(command_features)\n",
    "\n",
    "# One-hot encode labels\n",
    "num_classes = len(command_mapping)\n",
    "command_labels = to_categorical(command_labels, num_classes=num_classes)\n",
    "\n",
    "logging.info(f'Command mapping: {command_mapping}')\n",
    "\n",
    "# Prepare data for boundary detection\n",
    "windows, labels = create_boundary_detection_data(annotations, data_dir)\n",
    "\n",
    "# Reshape data for the CNN\n",
    "windows = windows.reshape((windows.shape[0], windows.shape[1], windows.shape[2], 1))\n",
    "\n",
    "# Build and train the boundary detection model\n",
    "input_shape = (windows.shape[1], windows.shape[2], 1)\n",
    "boundary_model = build_boundary_detection_model(input_shape)\n",
    "logging.info('Training boundary detection model...')\n",
    "boundary_model.fit(windows, labels, epochs=10, batch_size=32, validation_split=0.2)\n",
    "logging.info('Boundary detection model trained.')\n",
    "\n",
    "# Build and train the command recognition model\n",
    "input_shape = (command_features.shape[1], command_features.shape[2], 1)\n",
    "command_model = build_command_recognition_model(input_shape, num_classes)\n",
    "logging.info('Training command recognition model...')\n",
    "command_model.fit(command_features, command_labels, epochs=10, batch_size=32, validation_split=0.2)\n",
    "logging.info('Command recognition model trained.')\n",
    "\n",
    "# Detect commands in a new audio file\n",
    "new_audio_path = f'{data_dir}/scenes/wav/2015_speech_true_Ofen_aus_Alarm_an.wav'\n",
    "logging.info(f'Detecting commands in {new_audio_path}...')\n",
    "detected_commands = detect_command_pairs(new_audio_path, boundary_model, command_model)\n",
    "\n",
    "# Print recognized commands\n",
    "logging.info('Detected commands:')\n",
    "for segment, command in detected_commands:\n",
    "    print(command)\n"
   ],
   "id": "f8b094ed7c9a39e0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 08:41:45,749 - INFO - Loading annotations...\n",
      "2024-06-04 08:41:45,756 - INFO - Annotations loaded.\n",
      "2024-06-04 08:41:45,758 - INFO - Preparing command data...\n",
      "  7%|▋         | 78/1065 [00:00<00:11, 84.47it/s]/usr/local/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=800\n",
      "  warnings.warn(\n",
      "100%|██████████| 1065/1065 [00:12<00:00, 88.11it/s]\n",
      "2024-06-04 08:41:58,111 - INFO - Command data prepared.\n",
      "2024-06-04 08:41:58,211 - INFO - Command mapping: {'Licht aus': 0, 'Ofen an': 1, 'Radio an': 2, 'Fernseher an': 3, 'Heizung aus': 4, 'Alarm an': 5, 'Lüftung aus': 6, 'Staubsauger aus': 7, 'Heizung an': 8, 'Staubsauger an': 9, 'Alarm aus': 10, 'Licht an': 11, 'Ofen aus': 12, 'Radio aus': 13, 'Lüftung an': 14, 'Fernseher aus': 15}\n",
      "2024-06-04 08:41:58,212 - INFO - Creating boundary detection data...\n",
      " 59%|█████▉    | 629/1065 [00:43<00:28, 15.28it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1f3701f9d756363"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
