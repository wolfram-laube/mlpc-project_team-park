{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T11:20:29.498075Z",
     "start_time": "2024-06-04T11:20:29.489221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the feature mapping\n",
    "data_dir = '../dataset'\n",
    "feature_mapping_file = f'{data_dir}/meta/idx_to_feature_name.csv'\n",
    "feature_mapping = pd.read_csv(feature_mapping_file)\n",
    "print(feature_mapping.head())\n",
    "\n",
    "# Load an example feature file\n",
    "example_feature_file = f'{data_dir}/scenes/npy/9_speech_true_Radio_aus.npy'\n",
    "features = np.load(example_feature_file)\n",
    "print(f'Feature shape: {features.shape}')\n",
    "print(features)\n"
   ],
   "id": "1f3701f9d756363",
   "execution_count": 19,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T06:45:27.485425Z",
     "start_time": "2024-06-05T06:15:12.163847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "data_dir = '../dataset'\n",
    "annotations_file = f'{data_dir}/development_scene_annotations.csv'\n",
    "model_save_path = 'best_command_model.keras'\n",
    "\n",
    "# Load annotations\n",
    "logging.info('Loading annotations...')\n",
    "annotations = pd.read_csv(annotations_file)\n",
    "logging.info('Annotations loaded.')\n",
    "\n",
    "# Check class distribution\n",
    "class_counts = annotations['command'].value_counts()\n",
    "plt.figure(figsize=(12, 6))\n",
    "class_counts.plot(kind='bar')\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Commands')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "def prepare_feature_data(annotations, data_dir, feature_dir):\n",
    "    command_features = []\n",
    "    command_labels = []\n",
    "    command_mapping = {}  # Mapping of command texts to numerical labels\n",
    "    current_label = 0\n",
    "    max_len = 0  # To determine the maximum length of features\n",
    "\n",
    "    logging.info('Preparing command data...')\n",
    "    for index, row in tqdm(annotations.iterrows(), total=annotations.shape[0]):\n",
    "        feature_path = os.path.join(feature_dir, row['filename'] + '.npy')\n",
    "        features = np.load(feature_path)\n",
    "        max_len = max(max_len, features.shape[1])  # Update max_len\n",
    "        \n",
    "        command_text = row['command']\n",
    "        if command_text not in command_mapping:\n",
    "            command_mapping[command_text] = current_label\n",
    "            current_label += 1\n",
    "        \n",
    "        command_label = command_mapping[command_text]\n",
    "        \n",
    "        command_features.append(features)\n",
    "        command_labels.append(command_label)\n",
    "\n",
    "    # Pad features to the same length\n",
    "    padded_features = []\n",
    "    for feature in command_features:\n",
    "        pad_width = max_len - feature.shape[1]\n",
    "        if pad_width > 0:\n",
    "            feature = np.pad(feature, ((0, 0), (0, pad_width)), mode='constant')\n",
    "        padded_features.append(feature)\n",
    "    \n",
    "    logging.info('Command data prepared.')\n",
    "    return np.array(padded_features), np.array(command_labels), command_mapping\n",
    "\n",
    "# Prepare feature-based command data\n",
    "feature_dir = f'{data_dir}/scenes/npy'\n",
    "command_features, command_labels, command_mapping = prepare_feature_data(annotations, data_dir, feature_dir)\n",
    "\n",
    "# Normalize features across each feature dimension\n",
    "command_features = (command_features - np.mean(command_features, axis=(0, 2), keepdims=True)) / np.std(command_features, axis=(0, 2), keepdims=True)\n",
    "\n",
    "# One-hot encode labels\n",
    "num_classes = len(command_mapping)\n",
    "command_labels = to_categorical(command_labels, num_classes=num_classes)\n",
    "\n",
    "logging.info(f'Command mapping: {command_mapping}')\n",
    "\n",
    "# Data Augmentation Function\n",
    "def augment_data(features, noise_factor=0.005):\n",
    "    noise = np.random.randn(*features.shape) * noise_factor\n",
    "    augmented_features = features + noise\n",
    "    augmented_features = np.clip(augmented_features, -1.0, 1.0)\n",
    "    return augmented_features\n",
    "\n",
    "# Augment the training data\n",
    "augmented_features = augment_data(command_features)\n",
    "combined_features = np.concatenate((command_features, augmented_features), axis=0)\n",
    "combined_labels = np.concatenate((command_labels, command_labels), axis=0)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(np.argmax(command_labels, axis=1)), y=np.argmax(command_labels, axis=1))\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# GRU with Attention Model\n",
    "def attention_block(inputs):\n",
    "    attention = layers.Dense(1, activation='tanh')(inputs)\n",
    "    attention = layers.Flatten()(attention)\n",
    "    attention = layers.Activation('softmax')(attention)\n",
    "    attention = layers.RepeatVector(inputs.shape[-1])(attention)\n",
    "    attention = layers.Permute([2, 1])(attention)\n",
    "    output_attention = layers.Multiply()([inputs, attention])\n",
    "    return output_attention\n",
    "\n",
    "def build_gru_attention_model(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.GRU(128, return_sequences=True)(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.GRU(128, return_sequences=True)(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = attention_block(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "input_shape = (command_features.shape[1], command_features.shape[2])\n",
    "command_model = build_gru_attention_model(input_shape, num_classes)\n",
    "\n",
    "logging.info('Training command recognition model...')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint(model_save_path, save_best_only=True, monitor='val_loss')\n",
    "\n",
    "history = command_model.fit(combined_features, combined_labels, epochs=50, batch_size=32, validation_split=0.2,\n",
    "                            callbacks=[early_stopping, model_checkpoint], class_weight=class_weights)\n",
    "logging.info('Command recognition model trained.')\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n"
   ],
   "id": "32389aabb5c5f0dc",
   "execution_count": 47,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T07:19:51.612447Z",
     "start_time": "2024-06-05T07:19:35.295837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "data_dir = '../dataset'\n",
    "model_save_path = 'best_command_model.keras'\n",
    "annotations_file = f'{data_dir}/development_scene_annotations.csv'\n",
    "\n",
    "# Load annotations to get command mapping\n",
    "logging.info('Loading annotations...')\n",
    "annotations = pd.read_csv(annotations_file)\n",
    "logging.info('Annotations loaded.')\n",
    "\n",
    "# Extract command mapping\n",
    "command_mapping = {}\n",
    "current_label = 0\n",
    "for command in annotations['command'].unique():\n",
    "    command_mapping[command] = current_label\n",
    "    current_label += 1\n",
    "\n",
    "# Reverse command mapping\n",
    "reverse_command_mapping = {v: k for k, v in command_mapping.items()}\n",
    "\n",
    "# Load the best model\n",
    "logging.info('Loading the best model...')\n",
    "command_model = tf.keras.models.load_model(model_save_path)\n",
    "logging.info('Model loaded.')\n",
    "\n",
    "\n",
    "# Function to prepare feature data (from the training script)\n",
    "def prepare_feature_data(annotations, data_dir, feature_dir):\n",
    "    command_features = []\n",
    "    command_labels = []\n",
    "    command_mapping = {}  # Mapping of command texts to numerical labels\n",
    "    current_label = 0\n",
    "    max_len = 0  # To determine the maximum length of features\n",
    "\n",
    "    logging.info('Preparing command data...')\n",
    "    for index, row in tqdm(annotations.iterrows(), total=annotations.shape[0]):\n",
    "        feature_path = os.path.join(feature_dir, row['filename'] + '.npy')\n",
    "        features = np.load(feature_path)\n",
    "        max_len = max(max_len, features.shape[1])  # Update max_len\n",
    "\n",
    "        command_text = row['command']\n",
    "        if command_text not in command_mapping:\n",
    "            command_mapping[command_text] = current_label\n",
    "            current_label += 1\n",
    "\n",
    "        command_label = command_mapping[command_text]\n",
    "\n",
    "        command_features.append(features)\n",
    "        command_labels.append(command_label)\n",
    "\n",
    "    # Pad features to the same length\n",
    "    padded_features = []\n",
    "    for feature in command_features:\n",
    "        pad_width = max_len - feature.shape[1]\n",
    "        if pad_width > 0:\n",
    "            feature = np.pad(feature, ((0, 0), (0, pad_width)), mode='constant')\n",
    "        padded_features.append(feature)\n",
    "\n",
    "    logging.info('Command data prepared.')\n",
    "    return np.array(padded_features), np.array(command_labels), command_mapping, max_len\n",
    "\n",
    "\n",
    "# Calculate mean, std, and max_len from training data\n",
    "feature_dir = f'{data_dir}/scenes/npy'\n",
    "annotations = pd.read_csv(annotations_file)\n",
    "command_features, _, _, max_len = prepare_feature_data(annotations, data_dir, feature_dir)\n",
    "mean = np.mean(command_features, axis=(0, 2), keepdims=True)\n",
    "std = np.std(command_features, axis=(0, 2), keepdims=True)\n",
    "\n",
    "\n",
    "# Function to prepare new data for prediction\n",
    "def prepare_new_data(new_feature_file, mean, std, max_len):\n",
    "    new_features = np.load(new_feature_file)\n",
    "    new_features = np.pad(new_features, ((0, 0), (0, max_len - new_features.shape[1])), mode='constant')\n",
    "    new_features = (new_features - mean) / std\n",
    "    #new_features = new_features.reshape(1, new_features.shape[0], new_features.shape[1])\n",
    "    return new_features\n",
    "\n",
    "\n",
    "# Function to predict and assess accuracy\n",
    "def predict_command(file_path):\n",
    "    new_features = prepare_new_data(file_path, mean, std, max_len)\n",
    "    predicted_command = command_model.predict(new_features)\n",
    "    predicted_label = np.argmax(predicted_command)\n",
    "    confidence = np.max(predicted_command)\n",
    "    predicted_command_text = reverse_command_mapping[predicted_label]\n",
    "    return predicted_command_text, confidence\n",
    "\n",
    "\n",
    "# Loop over random files and assess accuracy\n",
    "num_files_to_evaluate = 100\n",
    "random_files = random.sample(list(annotations['filename'].unique()), num_files_to_evaluate)\n",
    "\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "for file_name in random_files:\n",
    "    file_path = os.path.join(feature_dir, f\"{file_name}.npy\")\n",
    "    actual_command = annotations[annotations['filename'] == file_name]['command'].values[0]\n",
    "    predicted_command_text, confidence = predict_command(file_path)\n",
    "\n",
    "    logging.info(f\"File: {file_name}\")\n",
    "    logging.info(f\"Actual Command: {actual_command}\")\n",
    "    logging.info(f\"Predicted Command: {predicted_command_text}\")\n",
    "    logging.info(f\"Confidence: {confidence:.2f}\")\n",
    "\n",
    "    if actual_command == predicted_command_text:\n",
    "        correct_predictions += 1\n",
    "    total_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / total_predictions\n",
    "logging.info(f\"Overall Accuracy: {accuracy:.2%}\")\n"
   ],
   "id": "c5a1dcf3f55de3bf",
   "execution_count": 50,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "c0081edc5dde5243",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
