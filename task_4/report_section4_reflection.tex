\section{Critical Reflection}
Despite achieving good classification results for individual words and scenes with traditional approaches,
all models failed to correctly evaluate random audio snippets using the prediction script.
Extensive data augmentation and noise enrichment caused everything to be recognized as "noise,"
while less augmentation led to incorrect classifications (e.g., "Ofen aus" when no Ofen was involved).
This affected all models tested, including CNNs in Conv1D, Conv2D with or without LSTM, GRU, and Kolmogorov-Arnold-NN,
regardless of whether raw audio data or extracted features were used.

A successful approach involved using pre-trained Wav2Vec, fine-tuned on individual words and trained on scenes to generate transcriptions.
This method produced usable results even with challenging dialects and fast-spoken commands.
The transcriptions were filtered to meaningful commands using Levenshtein distance, resulting in reliable outcomes.
This approach bypasses traditional feature extraction and works directly with transcribed data.

\subsection{Real-World Deployment}
Our final system demonstrates promising performance in controlled environments.
However, deploying it in real-world scenarios would require addressing several challenges, including handling diverse accents,
background noise, and ensuring real-time processing capabilities.

\subsection{Adaptations for Real-World Use}
To fulfill real-world requirements, our system could be enhanced by integrating more advanced noise filtering techniques,
providing user customization options, and implementing continuous learning mechanisms to adapt to users' speech patterns over time.
Additionally, edge computing strategies could be employed to handle real-time processing needs.

\subsection{Conclusion}
The project successfully developed a speech command detection system that performs well in noisy environments.
Future work will focus on refining the system for real-world deployment, ensuring robustness, and enhancing user experience.
