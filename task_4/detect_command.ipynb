{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T06:05:03.960270Z",
     "start_time": "2024-06-02T06:05:03.957288Z"
    }
   },
   "cell_type": "code",
   "source": "data_dir = '../dataset'",
   "id": "4dbb6c8b5621c57f",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T06:28:31.145475Z",
     "start_time": "2024-06-02T06:28:30.189638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "\n",
    "# Function to pad or trim audio segments to a fixed length\n",
    "def pad_or_trim(segment, target_length):\n",
    "    if len(segment) > target_length:\n",
    "        return segment[:target_length]\n",
    "    elif len(segment) < target_length:\n",
    "        return np.pad(segment, (0, target_length - len(segment)), mode='constant')\n",
    "    else:\n",
    "        return segment\n",
    "\n",
    "# Function to process audio stream with sliding window\n",
    "def process_audio_stream(audio, sample_rate, word_model, command_model, window_size=1.0, stride=0.5, input_length=13632, framework='keras'):\n",
    "    window_samples = int(window_size * sample_rate)\n",
    "    stride_samples = int(stride * sample_rate)\n",
    "    current_position = 0\n",
    "    word_buffer = []\n",
    "    command_detections = []\n",
    "\n",
    "    while current_position + window_samples <= len(audio):\n",
    "        segment = audio[current_position:current_position + window_samples]\n",
    "        \n",
    "        # Pad or trim the segment to the required input length\n",
    "        segment = pad_or_trim(segment, input_length)\n",
    "        \n",
    "        # Ensure correct input shape for the model\n",
    "        segment_input = segment.reshape(1, -1)\n",
    "\n",
    "        if framework == 'keras':\n",
    "            # Word classification with Keras\n",
    "            word_prediction = word_model.predict(segment_input)\n",
    "        elif framework == 'pytorch':\n",
    "            # Word classification with PyTorch\n",
    "            segment_tensor = torch.tensor(segment_input, dtype=torch.float32)\n",
    "            word_prediction = word_model(segment_tensor).detach().numpy()\n",
    "\n",
    "        print(f'Word prediction: {word_prediction}')\n",
    "        predicted_word = np.argmax(word_prediction)  # Assuming the model returns class probabilities\n",
    "        print(f'Predicted word: {predicted_word}')\n",
    "\n",
    "        # Store the word and its timestamp if it is a recognized word\n",
    "        if predicted_word != 'other':  # Replace 'other' with your actual class for unrecognized words\n",
    "            timestamp = current_position / sample_rate\n",
    "            word_buffer.append((timestamp, predicted_word))\n",
    "\n",
    "            # Check if the buffered words form a recognized command\n",
    "            if len(word_buffer) > 1:\n",
    "                words = [w[1] for w in word_buffer]\n",
    "                command_audio = np.concatenate([audio[int(w[0] * sample_rate):int((w[0] + window_size) * sample_rate)] for w in word_buffer])\n",
    "                \n",
    "                # Pad or trim the command_audio to the required input length\n",
    "                command_audio = pad_or_trim(command_audio, input_length)\n",
    "                command_input = command_audio.reshape(1, -1)\n",
    "                \n",
    "                if framework == 'keras':\n",
    "                    command_prediction = command_model.predict(command_input)\n",
    "                elif framework == 'pytorch':\n",
    "                    command_tensor = torch.tensor(command_input, dtype=torch.float32)\n",
    "                    command_prediction = command_model(command_tensor).detach().numpy()\n",
    "                \n",
    "                print(f'Predicted command: {command_prediction}')\n",
    "                if command_prediction == 'recognized_command':  # Replace with your actual command class\n",
    "                    command_detections.append((word_buffer[0][0], ' '.join(words)))\n",
    "                    word_buffer = []  # Clear the buffer after recognizing a command\n",
    "        \n",
    "        current_position += stride_samples\n",
    "\n",
    "    return command_detections\n",
    "\n",
    "# Load your models\n",
    "# Assuming `word_model` and `command_model` are already loaded with trained models\n",
    "# For Keras:\n",
    "word_model = load_model(f'{data_dir}/word_model.h5')\n",
    "command_model = load_model(f'{data_dir}/command_model.h5')\n",
    "\n",
    "# For PyTorch:\n",
    "# word_model = torch.load(f'{data_dir}/word_model.pth')\n",
    "# command_model = torch.load(f'{data_dir}/command_model.pth')\n",
    "# word_model.eval()\n",
    "# command_model.eval()\n",
    "\n",
    "# Load the WAV file\n",
    "wav_file = f'{data_dir}/scenes/wav/2023_speech_true_Licht_an.wav'\n",
    "audio, sample_rate = librosa.load(wav_file, sr=None)\n",
    "\n",
    "# Process the audio stream\n",
    "detections = process_audio_stream(audio, sample_rate, word_model, command_model)\n",
    "\n",
    "# Output results\n",
    "for detection in detections:\n",
    "    print(f\"Detected command '{detection[1]}' at {detection[0]:.2f} seconds\")\n"
   ],
   "id": "d5fff48d34cc0aa0",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "302825fb94d3196d",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
