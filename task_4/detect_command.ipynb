{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T06:05:03.960270Z",
     "start_time": "2024-06-02T06:05:03.957288Z"
    }
   },
   "cell_type": "code",
   "source": "data_dir = '../dataset'",
   "id": "4dbb6c8b5621c57f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T06:28:31.145475Z",
     "start_time": "2024-06-02T06:28:30.189638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "\n",
    "# Function to pad or trim audio segments to a fixed length\n",
    "def pad_or_trim(segment, target_length):\n",
    "    if len(segment) > target_length:\n",
    "        return segment[:target_length]\n",
    "    elif len(segment) < target_length:\n",
    "        return np.pad(segment, (0, target_length - len(segment)), mode='constant')\n",
    "    else:\n",
    "        return segment\n",
    "\n",
    "# Function to process audio stream with sliding window\n",
    "def process_audio_stream(audio, sample_rate, word_model, command_model, window_size=1.0, stride=0.5, input_length=13632, framework='keras'):\n",
    "    window_samples = int(window_size * sample_rate)\n",
    "    stride_samples = int(stride * sample_rate)\n",
    "    current_position = 0\n",
    "    word_buffer = []\n",
    "    command_detections = []\n",
    "\n",
    "    while current_position + window_samples <= len(audio):\n",
    "        segment = audio[current_position:current_position + window_samples]\n",
    "        \n",
    "        # Pad or trim the segment to the required input length\n",
    "        segment = pad_or_trim(segment, input_length)\n",
    "        \n",
    "        # Ensure correct input shape for the model\n",
    "        segment_input = segment.reshape(1, -1)\n",
    "\n",
    "        if framework == 'keras':\n",
    "            # Word classification with Keras\n",
    "            word_prediction = word_model.predict(segment_input)\n",
    "        elif framework == 'pytorch':\n",
    "            # Word classification with PyTorch\n",
    "            segment_tensor = torch.tensor(segment_input, dtype=torch.float32)\n",
    "            word_prediction = word_model(segment_tensor).detach().numpy()\n",
    "\n",
    "        print(f'Word prediction: {word_prediction}')\n",
    "        predicted_word = np.argmax(word_prediction)  # Assuming the model returns class probabilities\n",
    "        print(f'Predicted word: {predicted_word}')\n",
    "\n",
    "        # Store the word and its timestamp if it is a recognized word\n",
    "        if predicted_word != 'other':  # Replace 'other' with your actual class for unrecognized words\n",
    "            timestamp = current_position / sample_rate\n",
    "            word_buffer.append((timestamp, predicted_word))\n",
    "\n",
    "            # Check if the buffered words form a recognized command\n",
    "            if len(word_buffer) > 1:\n",
    "                words = [w[1] for w in word_buffer]\n",
    "                command_audio = np.concatenate([audio[int(w[0] * sample_rate):int((w[0] + window_size) * sample_rate)] for w in word_buffer])\n",
    "                \n",
    "                # Pad or trim the command_audio to the required input length\n",
    "                command_audio = pad_or_trim(command_audio, input_length)\n",
    "                command_input = command_audio.reshape(1, -1)\n",
    "                \n",
    "                if framework == 'keras':\n",
    "                    command_prediction = command_model.predict(command_input)\n",
    "                elif framework == 'pytorch':\n",
    "                    command_tensor = torch.tensor(command_input, dtype=torch.float32)\n",
    "                    command_prediction = command_model(command_tensor).detach().numpy()\n",
    "                \n",
    "                print(f'Predicted command: {command_prediction}')\n",
    "                if command_prediction == 'recognized_command':  # Replace with your actual command class\n",
    "                    command_detections.append((word_buffer[0][0], ' '.join(words)))\n",
    "                    word_buffer = []  # Clear the buffer after recognizing a command\n",
    "        \n",
    "        current_position += stride_samples\n",
    "\n",
    "    return command_detections\n",
    "\n",
    "# Load your models\n",
    "# Assuming `word_model` and `command_model` are already loaded with trained models\n",
    "# For Keras:\n",
    "word_model = load_model(f'{data_dir}/word_model.h5')\n",
    "command_model = load_model(f'{data_dir}/command_model.h5')\n",
    "\n",
    "# For PyTorch:\n",
    "# word_model = torch.load(f'{data_dir}/word_model.pth')\n",
    "# command_model = torch.load(f'{data_dir}/command_model.pth')\n",
    "# word_model.eval()\n",
    "# command_model.eval()\n",
    "\n",
    "# Load the WAV file\n",
    "wav_file = f'{data_dir}/scenes/wav/2023_speech_true_Licht_an.wav'\n",
    "audio, sample_rate = librosa.load(wav_file, sr=None)\n",
    "\n",
    "# Process the audio stream\n",
    "detections = process_audio_stream(audio, sample_rate, word_model, command_model)\n",
    "\n",
    "# Output results\n",
    "for detection in detections:\n",
    "    print(f\"Detected command '{detection[1]}' at {detection[0]:.2f} seconds\")\n"
   ],
   "id": "d5fff48d34cc0aa0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001B[1mInput 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 13632, but received input with shape (1, 10496)\u001B[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 13632), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 90\u001B[0m\n\u001B[1;32m     87\u001B[0m audio, sample_rate \u001B[38;5;241m=\u001B[39m librosa\u001B[38;5;241m.\u001B[39mload(wav_file, sr\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m     89\u001B[0m \u001B[38;5;66;03m# Process the audio stream\u001B[39;00m\n\u001B[0;32m---> 90\u001B[0m detections \u001B[38;5;241m=\u001B[39m \u001B[43mprocess_audio_stream\u001B[49m\u001B[43m(\u001B[49m\u001B[43maudio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mword_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcommand_model\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;66;03m# Output results\u001B[39;00m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m detection \u001B[38;5;129;01min\u001B[39;00m detections:\n",
      "Cell \u001B[0;32mIn[13], line 34\u001B[0m, in \u001B[0;36mprocess_audio_stream\u001B[0;34m(audio, sample_rate, word_model, command_model, window_size, stride, input_length, framework)\u001B[0m\n\u001B[1;32m     30\u001B[0m segment_input \u001B[38;5;241m=\u001B[39m segment\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m framework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mkeras\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;66;03m# Word classification with Keras\u001B[39;00m\n\u001B[0;32m---> 34\u001B[0m     word_prediction \u001B[38;5;241m=\u001B[39m \u001B[43mword_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43msegment_input\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m framework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpytorch\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;66;03m# Word classification with PyTorch\u001B[39;00m\n\u001B[1;32m     37\u001B[0m     segment_tensor \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(segment_input, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m    120\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m--> 122\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/site-packages/keras/src/layers/input_spec.py:227\u001B[0m, in \u001B[0;36massert_input_compatibility\u001B[0;34m(input_spec, inputs, layer_name)\u001B[0m\n\u001B[1;32m    222\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m axis, value \u001B[38;5;129;01min\u001B[39;00m spec\u001B[38;5;241m.\u001B[39maxes\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    223\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m shape[axis] \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m {\n\u001B[1;32m    224\u001B[0m             value,\n\u001B[1;32m    225\u001B[0m             \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    226\u001B[0m         }:\n\u001B[0;32m--> 227\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    228\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mInput \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minput_index\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m of layer \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    229\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mincompatible with the layer: expected axis \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maxis\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    230\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mof input shape to have value \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvalue\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    231\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut received input with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    232\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshape \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    233\u001B[0m             )\n\u001B[1;32m    234\u001B[0m \u001B[38;5;66;03m# Check shape.\u001B[39;00m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m spec\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mValueError\u001B[0m: Exception encountered when calling Sequential.call().\n\n\u001B[1mInput 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 13632, but received input with shape (1, 10496)\u001B[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 13632), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "302825fb94d3196d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
